{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mura_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimamd/Bone-Abnormality-Classifier-in-Keras/blob/Khaled-Branch/marmora_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "up1iA_3xOwax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "!unzip MURA-v1.1.zip\n",
        "!rm MURA-v1.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMY_KstJs079",
        "colab_type": "code",
        "outputId": "81e651d2-892e-41d6-9daf-0245f97e4a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall keras\n",
        "!pip3 install git+https://github.com/keras-team/keras\n",
        "!pip uninstall keras-preprocessing\n",
        "!pip3 install git+https://github.com/keras-team/keras-preprocessing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras-2.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.6/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.2.4\n",
            "Collecting git+https://github.com/keras-team/keras\n",
            "  Cloning https://github.com/keras-team/keras to /tmp/pip-req-build-s2r0p5f8\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: keras_applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.0.6)\n",
            "Requirement already satisfied: keras_preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.0.5)\n",
            "Building wheels for collected packages: Keras\n",
            "  Running setup.py bdist_wheel for Keras ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-vt6iqede/wheels/18/59/26/2a3c8d65212670e9526dcb6966eba15ee401e814aa74ca121d\n",
            "Successfully built Keras\n",
            "Installing collected packages: Keras\n",
            "Successfully installed Keras-2.2.4\n",
            "Uninstalling Keras-Preprocessing-1.0.5:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/Keras_Preprocessing-1.0.5.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-Preprocessing-1.0.5\n",
            "Collecting git+https://github.com/keras-team/keras-preprocessing\n",
            "  Cloning https://github.com/keras-team/keras-preprocessing to /tmp/pip-req-build-p_rayzk2\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras-Preprocessing==1.0.5) (1.14.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras-Preprocessing==1.0.5) (1.11.0)\n",
            "Building wheels for collected packages: Keras-Preprocessing\n",
            "  Running setup.py bdist_wheel for Keras-Preprocessing ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-gsj3smxu/wheels/d5/b2/e4/de499da9d3a3120b4dfed9fc8580bedb35066713d8623e305d\n",
            "Successfully built Keras-Preprocessing\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X4I_XUYdSlH3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rdHciFEs2Fs",
        "colab_type": "code",
        "outputId": "f8c2d1ec-e608-4dd2-e0a5-f8243a3aa920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls\n",
        "# RESTART RUNTIME BEFORE PROCEEDING"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D8ADE8fydpdM",
        "colab_type": "code",
        "outputId": "5a1fb683-38b0-4455-f8f9-47df8f5e4e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/70a2a39hiix6anm/train_paths_labels.csv\n",
        "!wget https://www.dropbox.com/s/8kfselmk1m3fphm/valid_paths_labels.csv"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-16 00:26:58--  https://www.dropbox.com/s/70a2a39hiix6anm/train_paths_labels.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/70a2a39hiix6anm/train_paths_labels.csv [following]\n",
            "--2019-01-16 00:26:58--  https://www.dropbox.com/s/raw/70a2a39hiix6anm/train_paths_labels.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc49c3074411694df6d4de454f0c.dl.dropboxusercontent.com/cd/0/inline/AZda0tRtQKKD9WijXu71tYsQ3YpcxxIDG4yQUb6O7AuOYEAcbXAsj-lrJNoyezQnWrV9X0rxwSvJEH8NRXYStlPtA81U6q1f11Lo-_5Uzu15geqdyR8hTvLZIt3eeh44xLt8qC7evPiTQRIA5MTsWZwAleWUab2vkvX_8YGdcF6LCDNpoPLrZK_mxgpZjXrhF54/file [following]\n",
            "--2019-01-16 00:26:58--  https://uc49c3074411694df6d4de454f0c.dl.dropboxusercontent.com/cd/0/inline/AZda0tRtQKKD9WijXu71tYsQ3YpcxxIDG4yQUb6O7AuOYEAcbXAsj-lrJNoyezQnWrV9X0rxwSvJEH8NRXYStlPtA81U6q1f11Lo-_5Uzu15geqdyR8hTvLZIt3eeh44xLt8qC7evPiTQRIA5MTsWZwAleWUab2vkvX_8YGdcF6LCDNpoPLrZK_mxgpZjXrhF54/file\n",
            "Resolving uc49c3074411694df6d4de454f0c.dl.dropboxusercontent.com (uc49c3074411694df6d4de454f0c.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc49c3074411694df6d4de454f0c.dl.dropboxusercontent.com (uc49c3074411694df6d4de454f0c.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2497047 (2.4M) [text/plain]\n",
            "Saving to: ‘train_paths_labels.csv’\n",
            "\n",
            "train_paths_labels. 100%[===================>]   2.38M  13.2MB/s    in 0.2s    \n",
            "\n",
            "2019-01-16 00:26:59 (13.2 MB/s) - ‘train_paths_labels.csv’ saved [2497047/2497047]\n",
            "\n",
            "--2019-01-16 00:27:00--  https://www.dropbox.com/s/8kfselmk1m3fphm/valid_paths_labels.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/8kfselmk1m3fphm/valid_paths_labels.csv [following]\n",
            "--2019-01-16 00:27:00--  https://www.dropbox.com/s/raw/8kfselmk1m3fphm/valid_paths_labels.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb370a5d03e9bae7bcc04305763.dl.dropboxusercontent.com/cd/0/inline/AZdOXXlSmq5KENCjjBTasutamiRKxHeWSv_kHDiooMchQOS0lzKQ-IWiGLB3rYR-oz8ITH92M52lR2lswZLo-vKZj9ncf_AowzEsPsdeL4DSwX4Qvz3qaEJpHu1vRQDvzyMtRUZIMP6N0HBwS_3DbeOCHfv8oNuXKPKfAM0hUnGhSpmD1edIBGzFDyA7PCicZGc/file [following]\n",
            "--2019-01-16 00:27:00--  https://ucb370a5d03e9bae7bcc04305763.dl.dropboxusercontent.com/cd/0/inline/AZdOXXlSmq5KENCjjBTasutamiRKxHeWSv_kHDiooMchQOS0lzKQ-IWiGLB3rYR-oz8ITH92M52lR2lswZLo-vKZj9ncf_AowzEsPsdeL4DSwX4Qvz3qaEJpHu1vRQDvzyMtRUZIMP6N0HBwS_3DbeOCHfv8oNuXKPKfAM0hUnGhSpmD1edIBGzFDyA7PCicZGc/file\n",
            "Resolving ucb370a5d03e9bae7bcc04305763.dl.dropboxusercontent.com (ucb370a5d03e9bae7bcc04305763.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to ucb370a5d03e9bae7bcc04305763.dl.dropboxusercontent.com (ucb370a5d03e9bae7bcc04305763.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217083 (212K) [text/plain]\n",
            "Saving to: ‘valid_paths_labels.csv’\n",
            "\n",
            "valid_paths_labels. 100%[===================>] 212.00K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-01-16 00:27:01 (2.29 MB/s) - ‘valid_paths_labels.csv’ saved [217083/217083]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ml6cgXJH7Eux",
        "colab_type": "code",
        "outputId": "029083f5-964b-47cc-a4c6-8023af2df399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -r Dropbox-Uploader\n",
        "!rm dropbox_uploader.sh\n",
        "!rm valid_paths_labels.csv\n",
        "!rm train_paths_labels.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Dropbox-Uploader': No such file or directory\n",
            "rm: cannot remove 'dropbox_uploader.sh': No such file or directory\n",
            "rm: cannot remove 'valid_paths_labels.csv': No such file or directory\n",
            "rm: cannot remove 'train_paths_labels.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T_91u16WOSr3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, time, signal, shutil\n",
        "import multiprocessing as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input,Flatten\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras import backend as K\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.merge import concatenate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ez9S9xLB0yIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "# onlyfiles = [f for f in listdir(\"MURA-v1.1\")]\n",
        "# print(onlyfiles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N790OUoYiuDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ]
    },
    {
      "metadata": {
        "id": "cTqlisSN_xe0",
        "colab_type": "code",
        "outputId": "bc60fc2f-ae9c-4493-b261-912a5ac87f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!bash dropbox_uploader.sh download README.md"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > No such file or directory: /README.md\n",
            "Some error occured. Please check the log.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_FjBSeR09-UZ",
        "colab_type": "code",
        "outputId": "dff90d36-a7e1-4578-a97b-71f825e661e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "# just run the cell to be able to upload and download from Dropbox\n",
        "!git clone https://github.com/thatbrguy/Dropbox-Uploader.git\n",
        "!chmod +x Dropbox-Uploader/dropbox_uploader.sh\n",
        "source = 'Dropbox-Uploader'\n",
        "dest1 = '/content'\n",
        "shutil.move(source+'/'+'dropbox_uploader.sh', dest1)\n",
        "!echo \"EAQdVV4B6swAAAAAAAAMNeyKygEAA5FP7QaiPVRRgJwupFyPIyq0DaAP6LiVy23c\" > token.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Dropbox-Uploader' already exists and is not an empty directory.\n",
            "chmod: cannot access 'Dropbox-Uploader/dropbox_uploader.sh': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c1804bfc45c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Dropbox-Uploader'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'dropbox_uploader.sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo \"EAQdVV4B6swAAAAAAAAMNeyKygEAA5FP7QaiPVRRgJwupFyPIyq0DaAP6LiVy23c\" > token.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/dropbox_uploader.sh' already exists"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "iBG6GG75QHwc",
        "colab_type": "code",
        "outputId": "31b7a161-c6c2-45a0-97c8-fc07ac66caae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# download train and valid csv files that include images path and their labels\n",
        "!bash dropbox_uploader.sh download train_paths_labels.csv\n",
        "!bash dropbox_uploader.sh download valid_paths_labels.csv\n",
        "# the saved model file\n",
        "model_file= 'model.h5'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > No such file or directory: /train_paths_labels.csv\n",
            "Some error occured. Please check the log.\n",
            " > No such file or directory: /valid_paths_labels.csv\n",
            "Some error occured. Please check the log.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cu15H_TCAVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# watch_ monitors the state of certain file if it is modified and uploads it to Dropbox if so\n",
        "def watch_(file, interval):\n",
        "  from datetime import datetime\n",
        "  first_Time=False\n",
        "  while True:\n",
        "    if os.path.isfile(file):\n",
        "      if not first_Time:\n",
        "        os.system(\"bash dropbox_uploader.sh upload \"+file+\" \"+file)\n",
        "        first_Time=True\n",
        "      moddate = os.stat(file)[8]\n",
        "      time.sleep(interval)\n",
        "      moddate_ = os.stat(file)[8]\n",
        "      if moddate < moddate_:\n",
        "        os.system(\"bash dropbox_uploader.sh upload \"+file+\" \"+file)\n",
        "    else:\n",
        "      time.sleep(interval)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbXWb8jCVsGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MODEL"
      ]
    },
    {
      "metadata": {
        "id": "4Y4fjRr9Vwl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=True):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #default because refrenced before assignment error, just scroll down\n",
        "  base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "\n",
        "#   feature_Concatenation_Merged=None\n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 5): #Feature concatenated\n",
        "    base_model_new = Input(shape= (224, 224, 3))  \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "\n",
        "\n",
        "  if(add_denses):\n",
        "    \n",
        "\n",
        "    if(base ==5):  #Feature concatenated\n",
        "      \n",
        "      print(\"Adding bases\")\n",
        "      feature_Concatenation_model1=NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False)(base_model_new)\n",
        "      feature_Concatenation_model2=MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)(base_model_new)\n",
        "\n",
        "      x1 = feature_Concatenation_model1.output\n",
        "      x1 = GlobalAveragePooling2D()(x1)\n",
        "   \n",
        "      x2 = feature_Concatenation_model2.output\n",
        "      x2 = GlobalAveragePooling2D()(x2)\n",
        "      \n",
        "\n",
        " \n",
        "\n",
        "      merge = concatenate([x1, x2])\n",
        "      \n",
        "      \n",
        "      merge = Dense(512, activation='relu')(merge)\n",
        "      merge = Dense(128, activation='relu')(merge)\n",
        "      \n",
        "      predictions = Dense(1, activation='sigmoid')(merge)\n",
        "      \n",
        "      print(\"Model Creation\")\n",
        "\n",
        "      feature_Concatenation_Merged = Model(inputs=[base_model_new],outputs=predictions)\n",
        "      model= feature_Concatenation_Merged\n",
        "      print(\"all here done\")\n",
        "\n",
        "      \n",
        "    else:\n",
        "            \n",
        "      # add a global spatial average pooling layer\n",
        "      x = base_model.output\n",
        "      x = GlobalAveragePooling2D()(x)\n",
        "      \n",
        "      # let's add a fully-connected layer\n",
        "      #x = Dense(1024, activation='relu')(x)\n",
        "      x = Dense(512, activation='relu')(x)\n",
        "      x = Dense(128, activation='relu')(x)\n",
        "      #x = Dense(32, activation='relu')(x)\n",
        "      # and a logistic layer -- let's say we have 200 classes\n",
        "\n",
        "      predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "      # this is the model we will train\n",
        "      model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "  else:\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "  \n",
        "  \n",
        "  plot_model(model,to_file='demo.png',show_shapes=True)\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2nG7OGT2iwb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "bug: if called fn on another base model it still loads the model"
      ]
    },
    {
      "metadata": {
        "id": "bEX9fTsAO5jT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#KAREEM'S COPY OF FUNCTION JUST COMMENT THE CELL AND EVERYTHING IS NORMAL\n",
        "def evaluate_limps(model=1,epoch=5,batch=32, imagenet=True, freeze_all=False,verbose=2):\n",
        "  \n",
        "  df_train=pd.read_csv('train_paths_labels.csv')\n",
        "  df_valid=pd.read_csv('valid_paths_labels.csv')\n",
        "  \n",
        "  datagen = ImageDataGenerator(  rescale=1./255,\n",
        "    featurewise_center=True,  #CHANGED IT TO TRUE # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=True,  #CHANGED IT TO TRUE# divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode='nearest')\n",
        "  \n",
        "  datagen.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "  datagen.std  = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "  train_generator=datagen.flow_from_dataframe(dataframe=df_train, directory=None,x_col=\"Img_Path\",\n",
        "                                              y_col=\"Label\", class_mode=\"binary\", target_size=(224,224), batch_size=batch)\n",
        "  valid_generator=datagen.flow_from_dataframe(dataframe=df_valid, directory=None,x_col=\"Img_Path\",\n",
        "                                              y_col=\"Label\", class_mode=\"binary\", target_size=(224,224), batch_size=batch)\n",
        "  print(len(train_generator))\n",
        "  print(\"making model\")\n",
        "  if not os.path.isfile(model_file):\n",
        "    model=make_FT_model(base= model, imagenet=imagenet, freeze_all=freeze_all, add_denses=True)\n",
        "  else:\n",
        "    model= load_model(model_file, compile=False)\n",
        "  \n",
        "  print(\"compiling\")\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  checkpoint= ModelCheckpoint(model_file, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint]\n",
        "    \n",
        "  step_train=train_generator.n//train_generator.batch_size\n",
        "  step_valid=valid_generator.n//valid_generator.batch_size\n",
        "  model.fit_generator(generator=train_generator, steps_per_epoch=step_train, epochs=epoch,\n",
        "                      validation_data=valid_generator, validation_steps=step_valid, shuffle=True,\n",
        "                      verbose=verbose, callbacks=callbacks_list)\n",
        "   #\n",
        "  \n",
        "  \n",
        "  loss_tr, accuracy_tr =model.evaluate_generator(train_generator, use_multiprocessing=True,steps=step_train)\n",
        "  print(\"training loss/accuracy: \", loss_tr,'/', accuracy_tr)\n",
        "\n",
        "  loss_val, accuracy_val = model.evaluate_generator(valid_generator, use_multiprocessing=True,steps=step_valid)\n",
        "  print(\"validation loss/accuracy: \", loss_val,'/', accuracy_val)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxzeaLRCIIlf",
        "colab_type": "code",
        "outputId": "06e22244-e181-4462-9dbb-fa4951506c22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!rm model.h5"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'model.h5': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QENRLRsCkKvE",
        "colab_type": "code",
        "outputId": "7f79a0c6-d304-44e6-b613-87970eaf47fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropbox-Uploader     sample_data\t       valid_paths_labels.csv.1\n",
            "dropbox_uploader.sh  token.txt\n",
            "MURA-v1.1\t     train_paths_labels.csv.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uusJ6dlIjLzQ",
        "colab_type": "code",
        "outputId": "e643a1bc-82ac-43ce-c668-ad1e82ad4b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "p = mp.Process(target=watch_, args=(model_file,5))\n",
        "p.start()\n",
        "evaluate_limps(model=5,epoch=1,batch=32,imagenet=True,freeze_all=False,verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 36808 images belonging to 2 classes.\n",
            "Found 3197 images belonging to 2 classes.\n",
            "1151\n",
            "making model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}