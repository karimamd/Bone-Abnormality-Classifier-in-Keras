{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand_simple_ungeneralized.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cuxc9GEo-3A2",
        "colab_type": "code",
        "outputId": "03946511-296a-4b8f-8a03-9fb1eebfe603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xhZmTueLBAQr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fVlucxL-3BI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# !wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "# !unzip MURA-v1.1.zip\n",
        "# !rm MURA-v1.1.zip\n",
        "# !ls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IpYjxR_e-3BR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#leave this here don't ask why\n",
        "drivee=\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SstnRAgK-3Bc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ## If the drive thing didn't work unzip the downloaded files to a folder and give its name to directory\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# drivee=\"drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-xVml-5g-3Bj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "pd.options.display.max_colwidth = 100\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JndI8_Np-3Bq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paths_n_labels(csv,str_limp):\n",
        "    #make dataframe\n",
        "    print(csv)\n",
        "    studies=pd.read_csv(csv, sep=',',header=None)\n",
        "    #separate study paths and labels of given limp from those of other limps\n",
        "    limp_studies=studies[studies[0].str.contains(str_limp)==True]\n",
        "    #make it a numpy\n",
        "    limp_studies=np.array(limp_studies)\n",
        "    #limp study folder paths\n",
        "    limp_paths=[]\n",
        "    #labels of given limp\n",
        "    limp_labels=[]\n",
        "    for i in tqdm( range(limp_studies.shape[0]) ):\n",
        "        study_path= drivee + limp_studies[i][0]\n",
        "        study_label=limp_studies[i][1]\n",
        "        study_files = [f for f in listdir(study_path) if isfile(join(study_path, f))]\n",
        "        for image in study_files:\n",
        "            limp_paths.append(study_path + image)\n",
        "            limp_labels.append(study_label)\n",
        "\n",
        "    limp_paths=np.array(limp_paths)\n",
        "    limp_labels=np.array(limp_labels)\n",
        "\n",
        "    return limp_paths,limp_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTnA492y-3Bv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#general function with options for wrist data case, only set wrist_train=True in the case of wrist training data only\n",
        "#for all other limps and for validation data even that of wrist just pass the paths\n",
        "#targ_size is image resizing with default(224,224)\n",
        "#preprocess flag is for using keras preprocessing for images or just resizing\n",
        "def read_images(paths ,targ_size= (224, 224), wrist_train=False, preprocess=False):\n",
        "    images=[]\n",
        "    #load any limp images\n",
        "    if(not wrist_train):\n",
        "        for path in tqdm(paths):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size )\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    #special case for wrist train corrupted data       \n",
        "    else:\n",
        "        #did this because it gave an error at sample  5307 or near it if took all\n",
        "        sample_e=5307\n",
        "        sample_s2=5339\n",
        "        images=[]\n",
        "        for path in tqdm(paths[:sample_e]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "        #new start\n",
        "        for path in tqdm(paths[sample_s2:]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "    #making it a numpy array instead of python list\n",
        "    return (np.array(images))\n",
        "  \n",
        "\n",
        "def wrist_labels(labels):\n",
        "  sample_e=5307\n",
        "  sample_s2=5339\n",
        "  return np.hstack( [ labels[:sample_e], labels[sample_s2:] ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SR6nGnyJ-3B5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_studies=drivee+'MURA-v1.1/train_labeled_studies.csv'\n",
        "valid_studies=drivee+'MURA-v1.1/valid_labeled_studies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUPzv6QV-3CA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def images_n_labels(limp,preprocess=True):\n",
        "  \n",
        "  print(\"\\nreading studies of \"+ limp + \"\\n\")\n",
        "  print(train_studies)\n",
        "  train_paths,train_labels=paths_n_labels(train_studies,limp)\n",
        "  valid_paths,valid_labels=paths_n_labels(valid_studies,limp)\n",
        "  \n",
        "  print(train_labels.shape)\n",
        "  print(valid_labels.shape)\n",
        "  print(\"reading \"+ limp + \" training images\")\n",
        "  if (limp == \"WRIST\"):\n",
        "    train_labels=wrist_labels(train_labels)\n",
        "    train_imgs= read_images(train_paths,preprocess=preprocess, wrist_train=True)\n",
        "    \n",
        "  else:\n",
        "    train_imgs= read_images(train_paths,preprocess=preprocess, wrist_train=False)\n",
        "  print(train_imgs.shape)  \n",
        "  print(\"reading \"+ limp + \" validation images\")\n",
        "  valid_imgs= read_images(valid_paths,preprocess=True)\n",
        "  print(valid_imgs.shape)\n",
        "  \n",
        "  return train_imgs, train_labels, valid_imgs, valid_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-4Ivzbl-3CL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=1):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #default because refrenced before assignment error, just scroll down\n",
        "  base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  \n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "    \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  if(add_denses > 0):\n",
        "      # let's add a fully-connected layer\n",
        "      #x = Dense(1024, activation='relu')(x)\n",
        "      x = Dense(512, activation='relu')(x)\n",
        "  if(add_denses > 1):  \n",
        "      x = Dense(128, activation='relu')(x)\n",
        "      #x = Dense(32, activation='relu')(x)\n",
        "      # and a logistic layer -- let's say we have 200 classes\n",
        "  predictions = Dense(1, activation='sigmoid')(x)\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  if(add_denses == 0):\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGnNVVGg-3Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#parameter setting for experiment\n",
        "#choose here which limps to use when running the function\n",
        "#limps=[ \"ELBOW\", \"FINGER\", \"FOREARM\", \"HUMERUS\",\"HAND\", \"SHOULDER\", \"WRIST\"]\n",
        "LIMPS=[\"HUMERUS\"]\n",
        "\n",
        "# which Transfer Learning base models to use\n",
        "BASE=[0]\n",
        "#this is just to know index of each model when choosing base above and to print it when running the function\n",
        "bases=[\"MobileNetV2\", \"DenseNet169\", \"InceptionV3\", \"ResNet50\",\"NASNetMobile\"]\n",
        "\n",
        "\n",
        "# whether to use imagenet weights of not\n",
        "IMAGENET=True\n",
        "\n",
        "\n",
        "# whether to freeze ALL layers of base model or not\n",
        "FREEZE=False\n",
        "\n",
        "\n",
        "# how much information to display about epochs and progress , 0= none , 1 is line per epoch\n",
        "VERBOSE=1\n",
        "\n",
        "\n",
        "# whether to preprocess input \n",
        "PREPROCESS=True\n",
        "\n",
        "\n",
        "#whether to augment data\n",
        "AUGMENTATION=False\n",
        "\n",
        "EPOCH=5\n",
        "BATCH=64\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jmhBT0q8-3CZ",
        "colab_type": "code",
        "outputId": "f78435d2-0447-4acd-ab18-557df7df67f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "limp=\"HUMERUS\"\n",
        "train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp, preprocess=PREPROCESS)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 592/592 [00:00<00:00, 22087.46it/s]\n",
            "100%|██████████| 135/135 [00:00<00:00, 19420.74it/s]\n",
            "  1%|▏         | 19/1272 [00:00<00:06, 183.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "reading studies of HUMERUS\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n",
            "MURA-v1.1/train_labeled_studies.csv\n",
            "MURA-v1.1/valid_labeled_studies.csv\n",
            "(1272,)\n",
            "(288,)\n",
            "reading HUMERUS training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1272/1272 [00:07<00:00, 177.54it/s]\n",
            "  6%|▋         | 18/288 [00:00<00:01, 178.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1272, 224, 224, 3)\n",
            "reading HUMERUS validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 288/288 [00:02<00:00, 111.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(288, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1JOLCRK8NJFZ",
        "colab_type": "code",
        "outputId": "73c5f76e-2068-41c9-f3f6-778d7712dacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#run this before runnning reading again\n",
        "print(train_labels[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g0xrSmeTM2mv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nWp4fwvI8KmF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(  rescale=1./255,\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "py2B8ply-3Cn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pass to model list of limps because if wanted to train on less\n",
        "#function outputs a dictionary or dataframe has train and val accuracies for each limp using a chosen model\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_limps(models=[1],epoch=5,batch=32, imagenet=True, freeze_all=False,v=1 ,\n",
        "                   limps=[\"WRIST\"], preprocess_ip=True, augment=False):\n",
        "    \n",
        "    accuracies={}\n",
        "    batch_aug_size=64\n",
        "    for model in models:\n",
        "        print(\"\\n\\n used base model: \\n\\n\"+bases[model])\n",
        "        for limp in limps:\n",
        "            #print(\"reading \"+ limp + \" images\\n\")\n",
        "            #train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp, preprocess=preprocess_ip)\n",
        "            \n",
        "            print(\"making model\")\n",
        "            model=make_FT_model(base= model, imagenet=imagenet, freeze_all=freeze_all, add_denses=1)\n",
        "            \n",
        "            print(\"compiling\")\n",
        "            model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            unique, counts = np.unique(train_labels, return_counts=True)\n",
        "            print(\"normal ,abnormal :\")\n",
        "            print(counts)\n",
        "            zero_w=counts[1]/np.sum(counts)\n",
        "            one_w=1-zero_w\n",
        "            print(\"zero and one weights\",zero_w,one_w)\n",
        "            \n",
        "            ##############################################################\n",
        "\n",
        "            if(augment): #dataaugmentation model fitting\n",
        "              print(\"Augmenting Input data\")\n",
        "              # # compute quantities required for featurewise normalization\n",
        "              # # (std, mean, and principal components if ZCA whitening is applied)\n",
        "\n",
        "              datagen.fit(train_imgs)\n",
        "              model.fit_generator(datagen.flow(train_imgs, train_labels, batch_size=batch_aug_size),\n",
        "                            steps_per_epoch=len(train_imgs) / (batch_aug_size), epochs=7,validation_data=(valid_imgs, valid_labels))\n",
        "              print(\"training augmentation calculations for \"+ limp)\n",
        "\n",
        "              loss_tr, accuracy_tr =model.evaluate_generator(datagen.flow(train_imgs,train_labels),steps=len(valid_imgs) / batch_aug_size)\n",
        "\n",
        "              print(\"calculating validation augmentation loss for \"+ limp)\n",
        "\n",
        "              loss_val, accuracy_val = model.evaluate_generator(datagen.flow(valid_imgs,valid_labels),steps=len(valid_imgs) / batch_aug_size)\n",
        "\n",
        "              accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "\n",
        "#               print(model.predict(valid_imgs, batch_size=batch_aug_size))\n",
        "              print(accuracies)\n",
        "\n",
        "            ###############################################################\n",
        "            else:\n",
        "\n",
        "              print(\"fitting\")\n",
        "              model.fit(train_imgs, train_labels, epochs=epoch, validation_data=(valid_imgs, valid_labels),\n",
        "                      shuffle=True, verbose=v, batch_size=batch, class_weight={0: zero_w, 1: one_w} )\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            y_pred=model.predict(valid_imgs)\n",
        "            y_pred[y_pred >=0.5]=1\n",
        "            y_pred[y_pred <0.5]=0\n",
        "            #     print(y_pred)\n",
        "            #y_pred=np.argmax(y_pred, axis=1)\n",
        "\n",
        "            y_pred_for_training=model.predict(train_imgs)\n",
        "            y_pred_for_training[y_pred_for_training >=0.5]=1\n",
        "            y_pred_for_training[y_pred_for_training < 0.5]=0\n",
        "\n",
        "            \n",
        "            print(\"second prediction vlayes\")\n",
        "            print(y_pred)\n",
        "\n",
        "            # Compute confusion matrix\n",
        "            cnf_matrix = confusion_matrix(valid_labels, y_pred)\n",
        "\n",
        "            cnf_matrix2 = confusion_matrix(train_labels, y_pred_for_training)\n",
        "\n",
        "            np.set_printoptions(precision=2)\n",
        "\n",
        "            cm_class_label = ['Normal','abnormal']\n",
        "            # Plot non-normalized confusion matrix\n",
        "            plt.figure()\n",
        "            plot_confusion_matrix(cnf_matrix,cm_class_label,title='Confusion matrix for valiadation set')\n",
        "            #plot_confusion_matrix(cnf_matrix2,cm_class_label,title='Confusion matrix for training set')\n",
        "\n",
        "            \n",
        "            print(\"training loss calculations for \"+ limp)\n",
        "            loss_tr, accuracy_tr =model.evaluate(x=train_imgs, y=train_labels, batch_size=BATCH, verbose=v)\n",
        "\n",
        "            print(\"calculating validation loss for \"+ limp)\n",
        "            loss_val, accuracy_val =model.evaluate(x=valid_imgs, y=valid_labels, batch_size=BATCH, verbose=v)\n",
        "\n",
        "            print(\"validation predictions for testing\")\n",
        "\n",
        "            #valid_preds=model.predict(valid_imgs)\n",
        "            valid_preds=y_pred\n",
        "            print( np.unique(y_pred) )\n",
        "\n",
        "            accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "            print(\"\\n \\n \"+ limp)\n",
        "            print(accuracies)\n",
        "            print(\"train ratio of abnormal predictions to # abnormal samples\")\n",
        "            print(np.sum( model.predict(train_imgs) )/np.sum(train_labels))\n",
        "            print(\"valid ratio of abnormal predictions\")\n",
        "            print(np.sum(valid_preds) / np.sum(valid_labels))\n",
        "            pd.DataFrame(accuracies).head(2)\n",
        "            \n",
        "            \n",
        "            \n",
        "    return accuracies\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0L_UM8lG-3Cv",
        "colab_type": "code",
        "outputId": "90760955-3346-44e2-a603-c82fbc50ff4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5977
        }
      },
      "cell_type": "code",
      "source": [
        "R=evaluate_limps(models=BASE,epoch=EPOCH,batch=BATCH, imagenet=IMAGENET, freeze_all=FREEZE,\n",
        "                 v=VERBOSE, limps=LIMPS, preprocess_ip=PREPROCESS, augment=AUGMENTATION)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " used base model: \n",
            "\n",
            "MobileNetV2\n",
            "making model\n",
            "compiling\n",
            "normal ,abnormal :\n",
            "[673 599]\n",
            "zero and one weights 0.47091194968553457 0.5290880503144655\n",
            "fitting\n",
            "Train on 1272 samples, validate on 288 samples\n",
            "Epoch 1/5\n",
            "1272/1272 [==============================] - 68s 54ms/step - loss: 0.2931 - acc: 0.7138 - val_loss: 0.5547 - val_acc: 0.7882\n",
            "Epoch 2/5\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.1841 - acc: 0.8451 - val_loss: 0.6938 - val_acc: 0.7604\n",
            "Epoch 3/5\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.1007 - acc: 0.9182 - val_loss: 1.7705 - val_acc: 0.6181\n",
            "Epoch 4/5\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.1151 - acc: 0.9041 - val_loss: 0.8882 - val_acc: 0.7500\n",
            "Epoch 5/5\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.0790 - acc: 0.9418 - val_loss: 0.9589 - val_acc: 0.7431\n",
            "second prediction vlayes\n",
            "[[0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Confusion matrix, without normalization\n",
            "[[104  44]\n",
            " [ 30 110]]\n",
            "training loss calculations for HUMERUS\n",
            "1272/1272 [==============================] - 4s 3ms/step\n",
            "calculating validation loss for HUMERUS\n",
            "288/288 [==============================] - 1s 3ms/step\n",
            "validation predictions for testing\n",
            "[0. 1.]\n",
            "\n",
            " \n",
            " HUMERUS\n",
            "{'HUMERUS': [0.93474842654834, 0.7430555555555556]}\n",
            "train ratio of abnormal predictions to # abnormal samples\n",
            "1.0561022113679048\n",
            "valid ratio of abnormal predictions\n",
            "1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAGACAYAAADf43dzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDV1FfeTzqQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f059067efd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "OeSkkiCA_4-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=1):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #default because refrenced before assignment error, just scroll down\n",
        "  base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  \n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "    \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  if(add_denses > 0):\n",
        "      # let's add a fully-connected layer\n",
        "      #x = Dense(1024, activation='relu')(x)\n",
        "      x = Dense(512, activation='relu')(x)\n",
        "  if(add_denses > 1):  \n",
        "      x = Dense(128, activation='relu')(x)\n",
        "      #x = Dense(32, activation='relu')(x)\n",
        "      # and a logistic layer -- let's say we have 200 classes\n",
        "  predictions = Dense(2, activation='softmax')(x)\n",
        "  # this is the model we will train\n",
        "  model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  if(add_denses == 0):\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Wd4VTkwJ7m7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels=keras.utils.to_categorical(train_labels, num_classes=2)\n",
        "valid_labels=keras.utils.to_categorical(valid_labels, num_classes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJ-BPtVGMr8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pass to model list of limps because if wanted to train on less\n",
        "#function outputs a dictionary or dataframe has train and val accuracies for each limp using a chosen model\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_limps(models=[1],epoch=5,batch=32, imagenet=True, freeze_all=False,v=1 ,\n",
        "                   limps=[\"WRIST\"], preprocess_ip=True, augment=False):\n",
        "    \n",
        "    accuracies={}\n",
        "    for model in models:\n",
        "        print(\"\\n\\n used base model: \\n\\n\"+bases[model])\n",
        "        for limp in limps:\n",
        "            #print(\"reading \"+ limp + \" images\\n\")\n",
        "            #train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp, preprocess=preprocess_ip)\n",
        "            \n",
        "            print(\"making model\")\n",
        "            model=make_FT_model(base= model, imagenet=imagenet, freeze_all=freeze_all, add_denses=1)\n",
        "            \n",
        "            print(\"compiling\")\n",
        "            model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            unique, counts = np.unique(train_labels[0], return_counts=True)\n",
        "            print(\"normal ,abnormal :\")\n",
        "            print(unique)\n",
        "            print(counts)\n",
        "            zero_w=counts[1]/np.sum(counts)\n",
        "            one_w=1-zero_w\n",
        "            print(\"zero and one weights\",zero_w,one_w)\n",
        "             \n",
        "            print(\"fitting\")\n",
        "            model.fit(train_imgs, train_labels, epochs=epoch, validation_data=(valid_imgs, valid_labels),\n",
        "                      shuffle=True, verbose=v, batch_size=batch, class_weight={0: 0.26772505863250945, 1: 0.7322749413674905} )\n",
        "            \n",
        "            \n",
        "            \n",
        "            y_pred=model.predict(valid_imgs)\n",
        "            #     print(y_pred)\n",
        "            y_pred=np.argmax(y_pred, axis=1)\n",
        "\n",
        "            y_pred_for_training=model.predict(valid_imgs)\n",
        "            y_pred_for_training=np.argmax(y_pred_for_training, axis=1)\n",
        "\n",
        "\n",
        "            print(\"second prediction vlayes\")\n",
        "            print(y_pred)\n",
        "\n",
        "            # Compute confusion matrix\n",
        "            cnf_matrix = confusion_matrix(valid_labels, y_pred)\n",
        "\n",
        "            cnf_matrix2 = confusion_matrix(train_labels, y_pred)\n",
        "\n",
        "            np.set_printoptions(precision=2)\n",
        "\n",
        "            cm_class_label = ['Normal','abnormal']\n",
        "            # Plot non-normalized confusion matrix\n",
        "            plt.figure()\n",
        "            plot_confusion_matrix(cnf_matrix,cm_class_label,title='Confusion matrix for valiadation set')\n",
        "            plot_confusion_matrix(cnf_matrix2,cm_class_label,title='Confusion matrix for training set')\n",
        "\n",
        "            \n",
        "            \n",
        "            print(\"training loss calculations for \"+ limp)\n",
        "            loss_tr, accuracy_tr =model.evaluate(x=train_imgs, y=train_labels, batch_size=BATCH, verbose=v)\n",
        "\n",
        "            print(\"calculating validation loss for \"+ limp)\n",
        "            loss_val, accuracy_val =model.evaluate(x=valid_imgs, y=valid_labels, batch_size=BATCH, verbose=v)\n",
        "\n",
        "            print(\"validation predictions for testing\")\n",
        "\n",
        "            valid_preds=model.predict(valid_imgs)\n",
        "            print( \"min \"+np.min(valid_preds)+\"max \"+np.max(valid_preds) )\n",
        "\n",
        "            accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "            print(\"\\n \\n \"+ limp)\n",
        "            print(accuracies)\n",
        "            print(\"train ratio of abnormal predictions to # abnormal samples\")\n",
        "            print(np.sum( model.predict(train_imgs) )/np.sum(train_labels))\n",
        "            print(\"valid ratio of abnormal predictions\")\n",
        "            print(np.sum(valid_preds) / np.sum(valid_labels))\n",
        "            pd.DataFrame(accuracies).head(2)\n",
        "    return accuracies\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MpTlQcDLM0oL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "R=evaluate_limps(models=BASE,epoch=EPOCH,batch=BATCH, imagenet=IMAGENET, freeze_all=FREEZE,\n",
        "                 v=VERBOSE, limps=LIMPS, preprocess_ip=PREPROCESS, augment=AUGMENTATION)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Gw25ff6J8UG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}