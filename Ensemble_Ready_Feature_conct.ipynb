{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Ensemble_Ready_Feature_conct.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karimamd/Bone-Abnormality-Classifier-in-Keras/blob/Khaled-Branch/Ensemble_Ready_Feature_conct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "up1iA_3xOwax",
        "colab_type": "code",
        "outputId": "8803dbd6-b427-4701-a0d9-11dcf56152aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# !wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-18 14:41:10--  https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3380245855 (3.1G) [application/zip]\n",
            "Saving to: ‘MURA-v1.1.zip’\n",
            "\n",
            "MURA-v1.1.zip         2%[                    ]  80.01M  9.14MB/s    eta 4m 52s ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c1q30MNoZrZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "import zipfile\n",
        "import time\n",
        "import concurrent.futures\n",
        "\n",
        "# timer func evaluates func/s running time in minutes\n",
        "def timer(f):\n",
        "\n",
        "    def inner(*args, **kwargs):\n",
        "        try:\n",
        "            t0 = time.time()\n",
        "            return f(*args, **kwargs)\n",
        "        finally:\n",
        "            t1 = time.time()\n",
        "            print(f.__name__, 'executed in', (t1 - t0) / 60, ' min/s')\n",
        "\n",
        "    return inner\n",
        "\n",
        "# fast unzip, usually 900% faster than normal \"unzip\" command\n",
        "# thanks to Peter Bengtsson <3: https://www.peterbe.com/\n",
        "\n",
        "@timer\n",
        "def fast_unzip(fn, dest):\n",
        "\n",
        "    def unzip_member(zf, name, dest):\n",
        "        while True:\n",
        "            try:\n",
        "                zf.extract(name, dest)\n",
        "                break\n",
        "            except OSError as e:\n",
        "                if e.errno != os.errno.EEXIST:\n",
        "                    raise\n",
        "                time.sleep(2)\n",
        "                pass\n",
        "\n",
        "    with open(fn, 'rb') as f:\n",
        "        zf = zipfile.ZipFile(f)\n",
        "        futures = []\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            for member in zf.infolist():\n",
        "                futures.append(executor.submit(unzip_member, zf,\n",
        "                               member.filename, dest))\n",
        "\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                future.result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-JgkBnialez",
        "colab_type": "code",
        "outputId": "2be13a1a-3dec-4009-8648-3246c2724077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "cell_type": "code",
      "source": [
        "fast_unzip('MURA-v1.1.zip','/content')\n",
        "!rm MURA-v1.1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fast_unzip executed in 8.018811543782552e-06  min/s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f9b51171038d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfast_unzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MURA-v1.1.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm MURA-v1.1.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b4ee0b2f2521>\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-b4ee0b2f2521>\u001b[0m in \u001b[0;36mfast_unzip\u001b[0;34m(fn, dest)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mzf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MURA-v1.1.zip'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DMY_KstJs079",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall keras -y\n",
        "!pip3 install git+https://github.com/keras-team/keras\n",
        "!pip uninstall keras-preprocessing -y\n",
        "!pip3 install git+https://github.com/keras-team/keras-preprocessing\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rdHciFEs2Fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # to force restart runtime\n",
        "# def restart_runtime():\n",
        "#   os.kill(os.getpid(), 9)\n",
        "  \n",
        "# try:\n",
        "#     restart_runtime\n",
        "# except NameError:\n",
        "#     print(\"already restarted!\")\n",
        "# else:\n",
        "#     restart_runtime()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_91u16WOSr3",
        "colab_type": "code",
        "outputId": "c5e52073-a092-4108-9ba5-cae7e04f4c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, time, signal, shutil\n",
        "import multiprocessing as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.applications.xception import Xception\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input,Flatten\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras import backend as K\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Elt6blzj0a3",
        "colab_type": "code",
        "outputId": "ba7d21ec-9b8d-4788-fc88-7c515b0891a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "total_size = 0\n",
        "# to get size of certain directory\n",
        "for path, dirs, files in os.walk('MURA-v1.1'):\n",
        "    for f in files:\n",
        "        fp = os.path.join(path, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "print(\"Directory size: \" + str(total_size/1024**3) + \" (Gigabyte)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory size: 3.1397332791239023 (Gigabyte)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZIfxBONz_4g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drivee=\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ez9S9xLB0yIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from os import listdir\n",
        "# from os.path import isfile, join\n",
        "# onlyfiles = [f for f in listdir(\"MURA-v1.1\")]\n",
        "# print(onlyfiles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SgFz9AwaBzBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Directory of saved models { form-width: \"30%\", display-mode: \"both\" }\n",
        "\n",
        "''' select \"unique name\" for \"your\" models directory where pre-trained/saved\n",
        "models will be uploaded to Dropbox if you want to apply any modifications \n",
        "such as preprocessing '''\n",
        "\n",
        "dir = \"khalodmodels\" #@param {type:\"string\"}\n",
        "dir = dir + \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N790OUoYiuDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SETUP \n",
        "### just run all cells in this section"
      ]
    },
    {
      "metadata": {
        "id": "_FjBSeR09-UZ",
        "colab_type": "code",
        "outputId": "fb662f62-3764-494b-e123-c2d263365a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "# just run the cell to be able to upload and download from Dropbox\n",
        "!git clone https://github.com/thatbrguy/Dropbox-Uploader.git\n",
        "!chmod +x Dropbox-Uploader/dropbox_uploader.sh\n",
        "source = 'Dropbox-Uploader'\n",
        "dest1 = '/content'\n",
        "shutil.move(source+'/'+'dropbox_uploader.sh', dest1)\n",
        "!echo \"pzFqTa1uWVAAAAAAAAAACtGcVohdBzoFRrwtLYsQCxhD9a7IjevvuOTNV8OWB2LX\" > token.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Dropbox-Uploader' already exists and is not an empty directory.\n",
            "chmod: cannot access 'Dropbox-Uploader/dropbox_uploader.sh': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-75ae0ecd3cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Dropbox-Uploader'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'dropbox_uploader.sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo \"pzFqTa1uWVAAAAAAAAAACtGcVohdBzoFRrwtLYsQCxhD9a7IjevvuOTNV8OWB2LX\" > token.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/dropbox_uploader.sh' already exists"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cTqlisSN_xe0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!bash dropbox_uploader.sh download README.md > log.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBG6GG75QHwc",
        "colab_type": "code",
        "outputId": "6c3eb355-3d9e-452d-9588-1e6514a650fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# download train and valid csv files that include images path and their labels\n",
        "!bash dropbox_uploader.sh -p download /labels_csv/train_paths_labels.csv\n",
        "!bash dropbox_uploader.sh -p download /labels_csv/valid_paths_labels.csv\n",
        "\n",
        "# create folder of pre-trained/saved models if it doesn't exist\n",
        "os.system(\"bash dropbox_uploader.sh mkdir /\"+ dir[:-1] +\" > log.txt\")\n",
        "\n",
        "# download folder of pre-trained/saved models\n",
        "os.system(\"bash dropbox_uploader.sh download \"+ dir[:-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Skipping file \"/labels_csv/train_paths_labels.csv\", file exists with the same hash\n",
            "> Skipping file \"/labels_csv/valid_paths_labels.csv\", file exists with the same hash\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cu15H_TCAVe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# watch_ monitors the state of certain file if it is modified and uploads it to Dropbox if so\n",
        "def watch_(file, interval):\n",
        "    first_Time = False\n",
        "    while True:\n",
        "        if os.path.isfile(dir + file):\n",
        "            if not first_Time:\n",
        "                os.system('bash dropbox_uploader.sh upload ' + dir\n",
        "                          + file + ' ' + dir + file)\n",
        "                first_Time = True\n",
        "                \n",
        "            moddate = os.stat(dir + file)[8]\n",
        "            time.sleep(interval)\n",
        "            moddate_ = os.stat(dir + file)[8]\n",
        "            \n",
        "            if moddate < moddate_:\n",
        "                os.system('bash dropbox_uploader.sh upload ' + dir\n",
        "                          + file + ' ' + dir + file)\n",
        "        else:\n",
        "            time.sleep(interval)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lckhF_XQeLN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYjo-6DBFJbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paths_n_labels(csv):\n",
        "    #make dataframe\n",
        "    print(csv)\n",
        "    studies=pd.read_csv(csv, sep=',',header=None)\n",
        "    #separate study paths and labels of given limp from those of other limps\n",
        "    limp_studies=studies[studies[0].str.contains(\"Khalodaaa\")==False]\n",
        "    #make it a numpy\n",
        "    limp_studies=np.array(limp_studies)\n",
        "    #limp study folder paths\n",
        "    limp_paths=[]\n",
        "    #labels of given limp\n",
        "    limp_labels=[]\n",
        "    for i in tqdm( range(limp_studies.shape[0]) ):\n",
        "        study_path= drivee + limp_studies[i][0]\n",
        "        study_label=limp_studies[i][1]\n",
        "        study_files = [f for f in listdir(study_path) if isfile(join(study_path, f))]\n",
        "        for image in study_files:\n",
        "            limp_paths.append(study_path + image)\n",
        "            limp_labels.append(study_label)\n",
        "\n",
        "    limp_paths=np.array(limp_paths)\n",
        "    limp_labels=np.array(limp_labels)\n",
        "\n",
        "    return limp_paths,limp_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0wPLQsDFexB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_images(paths ,targ_size= (224, 224)):\n",
        "    images=[]\n",
        "    #load any limp images\n",
        "    for path in tqdm(paths):\n",
        "        img=k_im_prep.load_img(path, target_size=targ_size )\n",
        "        images.append(np.array(img))\n",
        "\n",
        "    #making it a numpy array instead of python list\n",
        "    return (np.array(images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbXWb8jCVsGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MODEL"
      ]
    },
    {
      "metadata": {
        "id": "4Y4fjRr9Vwl1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=True):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #default because refrenced before assignment error, just scroll down\n",
        "  base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "\n",
        "#   feature_Concatenation_Merged=None\n",
        "  #initializing pretrained model\n",
        "  \n",
        "  print(\"Choose model is\",base)\n",
        "  \n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 6): #Feature concatenated\n",
        "    base_model_new = Input(shape= (224, 224, 3))  \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "\n",
        "\n",
        "  if(add_denses):\n",
        "    \n",
        "\n",
        "    if(base ==6):  #Feature concatenated\n",
        "      \n",
        "      print(\"Adding bases\")\n",
        "      feature_Concatenation_model1=NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False, input_tensor=base_model_new)\n",
        "      feature_Concatenation_model2=MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False, input_tensor=base_model_new)\n",
        "      feature_Concatenation_model3=ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False, input_tensor=base_model_new)\n",
        "\n",
        "      x1 = feature_Concatenation_model1.output\n",
        "      x1 = GlobalAveragePooling2D()(x1)\n",
        "   \n",
        "      x2 = feature_Concatenation_model2.output\n",
        "      x2 = GlobalAveragePooling2D()(x2)\n",
        "      \n",
        "      x3 = feature_Concatenation_model2.output\n",
        "      x3 = GlobalAveragePooling2D()(x3)\n",
        "      \n",
        " \n",
        "\n",
        "      merge = concatenate([x1, x2, x3])\n",
        "      \n",
        "      \n",
        "      merge = Dense(512, activation='relu')(merge)\n",
        "      merge = Dense(128, activation='relu')(merge)\n",
        "      \n",
        "      predictions = Dense(1, activation='sigmoid')(merge)\n",
        "      \n",
        "      print(\"Model Creation\")\n",
        "\n",
        "      feature_Concatenation_Merged = Model(inputs=[base_model_new],outputs=predictions)\n",
        "      model= feature_Concatenation_Merged\n",
        "      print(\"all here done\")\n",
        "\n",
        "      \n",
        "    else:\n",
        "            \n",
        "      # add a global spatial average pooling layer\n",
        "      x = base_model.output\n",
        "      x = GlobalAveragePooling2D()(x)\n",
        "      \n",
        "      # let's add a fully-connected layer\n",
        "      #x = Dense(1024, activation='relu')(x)\n",
        "      x = Dense(512, activation='relu')(x)\n",
        "      x = Dense(128, activation='relu')(x)\n",
        "      #x = Dense(32, activation='relu')(x)\n",
        "      # and a logistic layer -- let's say we have 200 classes\n",
        "\n",
        "      predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "      # this is the model we will train\n",
        "      model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "  else:\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "  \n",
        "  \n",
        "  plot_model(model,to_file='demo.png',show_shapes=True)\n",
        "#   SVG(model_to_dot(model).create(prog='’'dot’, format=’svg’))\n",
        "#   print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzAiJSX0TX-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "models_names=['MobileNetV2','DenseNet169','InceptionV3','ResNet50','NASNetMobile','Xception','Feature_Conct']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FlPKw5LXEOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras.models import load_model\n",
        "\n",
        "# @timer\n",
        "# def evaluate_limps(model=1,epoch=5,batch=32, imagenet=True, freeze_all=False,verbose=2):\n",
        "  \n",
        "  \n",
        "#   model_file= 'model_'+models_names[model]+'.h5'\n",
        "  \n",
        "#   df_train=pd.read_csv('train_paths_labels.csv')\n",
        "#   df_valid=pd.read_csv('valid_paths_labels.csv')\n",
        "  \n",
        "#   datagen=ImageDataGenerator(rescale=1./255)\n",
        "#   train_generator=datagen.flow_from_dataframe(dataframe=df_train, directory=None,x_col=\"Img_Path\", y_col=\"Label\", class_mode=\"binary\", target_size=(224,224), batch_size=batch)\n",
        "#   valid_generator=datagen.flow_from_dataframe(dataframe=df_valid, directory=None,x_col=\"Img_Path\", y_col=\"Label\", class_mode=\"binary\", target_size=(224,224), batch_size=batch)\n",
        "  \n",
        "#   keras.backend.clear_session()\n",
        "  \n",
        "  \n",
        "#   if not os.path.isfile(dir+model_file):\n",
        "#     print(\"making model\")\n",
        "#     model=make_FT_model(base= model, imagenet=imagenet, freeze_all=freeze_all, add_denses=True)\n",
        "#   else:\n",
        "#     print(\"loading saved model\")\n",
        "#     model= load_model(dir+model_file)\n",
        "  \n",
        "#   print(\"compiling\")\n",
        "#   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "#   checkpoint= ModelCheckpoint(dir+model_file, monitor='val_loss', verbose=1, save_best_only=False,\n",
        "#                               save_weights_only=False, mode='auto', period=1)\n",
        "#   callbacks_list = [checkpoint]\n",
        "    \n",
        "#   step_train=train_generator.n//train_generator.batch_size\n",
        "#   step_valid=valid_generator.n//valid_generator.batch_size\n",
        "  \n",
        "#   model.fit_generator(generator=train_generator, steps_per_epoch=step_train, epochs=epoch, validation_data=valid_generator,\n",
        "#                       validation_steps=step_valid, shuffle=True, verbose=verbose, callbacks=callbacks_list)\n",
        "  \n",
        "  \n",
        "  \n",
        "#   loss_tr, accuracy_tr =model.evaluate_generator(train_generator, use_multiprocessing=True,workers=5,steps=step_train)\n",
        "#   print(\"training loss/accuracy: \", loss_tr,'/', accuracy_tr)\n",
        "\n",
        "#   loss_val, accuracy_val = model.evaluate_generator(valid_generator, use_multiprocessing=True,workers=5,steps=step_valid)\n",
        "#   print(\"validation loss/accuracy: \", loss_val,'/', accuracy_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Atw91wISar9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Kareem's eval function"
      ]
    },
    {
      "metadata": {
        "id": "bEX9fTsAO5jT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#KAREEM'S COPY OF FUNCTION JUST COMMENT THE CELL AND EVERYTHING IS NORMAL\n",
        "def evaluate_limps(model=1,epoch=5,batch=32, imagenet=True, freeze_all=False,verbose=2):\n",
        "  \n",
        "  df_train=pd.read_csv('train_paths_labels.csv')\n",
        "  df_valid=pd.read_csv('valid_paths_labels.csv')\n",
        "  \n",
        "  datagen = ImageDataGenerator(  rescale=1./255,\n",
        "    featurewise_center=True,  #CHANGED IT TO TRUE # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=True,  #CHANGED IT TO TRUE# divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode='nearest')\n",
        "  \n",
        "  datagen.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "  datagen.std  = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "  train_generator=datagen.flow_from_dataframe(dataframe=df_train, directory=None,x_col=\"Img_Path\",\n",
        "                                              y_col=\"Label\", class_mode=\"binary\", target_size=(224,224), batch_size=batch)\n",
        "  valid_generator=datagen.flow_from_dataframe(dataframe=df_valid, directory=None,x_col=\"Img_Path\",\n",
        "                                              y_col=\"Label\", class_mode=\"binary\", target_size=(224,224), batch_size=batch)\n",
        "  print(len(train_generator))\n",
        "  print(\"making model\")\n",
        "  if not os.path.isfile(model_file):\n",
        "    model=make_FT_model(base= model, imagenet=imagenet, freeze_all=freeze_all, add_denses=True)\n",
        "  else:\n",
        "    model= load_model(model_file, compile=False)\n",
        "  \n",
        "  print(\"compiling\")\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  checkpoint= ModelCheckpoint(model_file, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "  callbacks_list = [checkpoint]\n",
        "    \n",
        "  step_train=train_generator.n//train_generator.batch_size\n",
        "  step_valid=valid_generator.n//valid_generator.batch_size\n",
        "  model.fit_generator(generator=train_generator, steps_per_epoch=step_train, epochs=epoch,\n",
        "                      validation_data=valid_generator, validation_steps=step_valid, shuffle=True,\n",
        "                      verbose=verbose, callbacks=callbacks_list)\n",
        " \n",
        "  \n",
        "  #Confession matrix part\n",
        "   ###########################################\n",
        "  \n",
        "  num_of_test_samples=3197\n",
        "  train_studies=drivee+'MURA-v1.1/train_labeled_studies.csv'\n",
        "  valid_studies=drivee+'MURA-v1.1/valid_labeled_studies.csv'\n",
        "  \n",
        "  \n",
        "  valid_paths,valid_labels=paths_n_labels(valid_studies)\n",
        "#   train_paths,train_labels=paths_n_labels(train_studies)\n",
        "\n",
        "#   train_imgs= read_images(train_paths)\n",
        "  valid_imgs= read_images(valid_paths)\n",
        "\n",
        "\n",
        "  \n",
        "  y_pred=model.predict(valid_imgs)\n",
        "  y_pred[y_pred >=0.5]=1\n",
        "  y_pred[y_pred <0.5]=0\n",
        "\n",
        "\n",
        "#   y_pred_for_training=model.predict(train_imgs)\n",
        "#   y_pred_for_training[y_pred_for_training >=0.5]=1\n",
        "#   y_pred_for_training[y_pred_for_training < 0.5]=0\n",
        "\n",
        "\n",
        "  print(\"second prediction vlayes\")\n",
        "  print(y_pred)\n",
        "\n",
        "  # Compute confusion matrix\n",
        "  cnf_matrix = confusion_matrix(valid_labels, y_pred)\n",
        "\n",
        "#   cnf_matrix2 = confusion_matrix(train_labels, y_pred_for_training)\n",
        "  \n",
        "  plot_matrix_multiples(cnf_matrix,Name='Confusion matrix for valiadation set')\n",
        "  \n",
        "#   plot_matrix_multiples(cnf_matrix2,Name='Confusion matrix for training set')\n",
        "\n",
        "  \n",
        "    \n",
        "  ###########################################\n",
        "  loss_tr, accuracy_tr =model.evaluate_generator(train_generator, use_multiprocessing=True,steps=step_train)\n",
        "  print(\"training loss/accuracy: \", loss_tr,'/', accuracy_tr)\n",
        "\n",
        "  loss_val, accuracy_val = model.evaluate_generator(valid_generator, use_multiprocessing=True,steps=step_valid)\n",
        "  print(\"validation loss/accuracy: \", loss_val,'/', accuracy_val)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iicD-VGd9oQA",
        "colab_type": "code",
        "outputId": "63ef95cb-3c3c-433e-f8e3-cc064e3194b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!bash dropbox_uploader.sh upload model_Feature_Conct.h5 khalodmodels/model_Feature_Conct.h5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " > No such file or directory: /content/model_Feature_Conct.h5\n",
            "Some error occured. Please check the log.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KEcCuFNP0TAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_matrix_multiples(cnf_matrix,Name):\n",
        "  \n",
        "  np.set_printoptions(precision=2)\n",
        "\n",
        "  cm_class_label = ['Normal','abnormal']\n",
        "  # Plot non-normalized confusion matrix\n",
        "  plt.figure()\n",
        "  plot_confusion_matrix(cnf_matrix,cm_class_label,title=Name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77y48PtKSdLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SubModel/s Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "QENRLRsCkKvE",
        "colab_type": "code",
        "outputId": "91bdd9ac-0223-4d42-b8c8-2422d5f8b602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "demo.png\t     log.txt\t  train_paths_labels.csv\n",
            "Dropbox-Uploader     MURA-v1.1\t  valid_paths_labels.csv\n",
            "dropbox_uploader.sh  README.md\n",
            "khalodmodels\t     sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uusJ6dlIjLzQ",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "e2a17fe8-3385-40fb-8556-489e720b4849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3192
        }
      },
      "cell_type": "code",
      "source": [
        "#@title SubModel Evaluation { form-width: \"30%\" }\n",
        "model = 6 #@param {type:\"number\"}\n",
        "epoch = 2 #@param {type:\"integer\"}\n",
        "batch_size = 32 #@param {type:\"integer\"}\n",
        "\n",
        "# No need now as I saved submodels and using them in ensemble model\n",
        "\n",
        "model_file= 'model_'+ models_names[model] +'.h5'\n",
        "\n",
        "p = mp.Process(target=watch_, args=(model_file,20))\n",
        "p.start()\n",
        "evaluate_limps(model=model,epoch=epoch,batch=batch_size,imagenet=True,freeze_all=True,verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 36808 images belonging to 2 classes.\n",
            "Found 3197 images belonging to 2 classes.\n",
            "1151\n",
            "making model\n",
            "compiling\n",
            "Epoch 1/2\n",
            "1150/1150 [==============================] - 1654s 1s/step - loss: 0.4702 - acc: 0.7880 - val_loss: 1.1526 - val_acc: 0.6452\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.15259, saving model to model_Feature_Conct.h5\n",
            "Epoch 2/2\n",
            "1150/1150 [==============================] - 1490s 1s/step - loss: 0.4604 - acc: 0.7946 - val_loss: 0.5812 - val_acc: 0.7330\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.15259 to 0.58121, saving model to model_Feature_Conct.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1199 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1199/1199 [00:00<00:00, 26621.97it/s]\u001b[A\n",
            "  0%|          | 0/3197 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1/valid_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  1%|          | 24/3197 [00:00<00:13, 232.88it/s]\u001b[A\n",
            "  2%|▏         | 51/3197 [00:00<00:12, 242.06it/s]\u001b[A\n",
            "  2%|▏         | 69/3197 [00:00<00:14, 217.15it/s]\u001b[A\n",
            "  3%|▎         | 93/3197 [00:00<00:14, 220.16it/s]\u001b[A\n",
            "  4%|▎         | 115/3197 [00:00<00:14, 219.04it/s]\u001b[A\n",
            "  4%|▍         | 134/3197 [00:00<00:14, 207.13it/s]\u001b[A\n",
            "  5%|▍         | 156/3197 [00:00<00:14, 207.89it/s]\u001b[A\n",
            "  6%|▌         | 179/3197 [00:00<00:14, 212.45it/s]\u001b[A\n",
            "  7%|▋         | 209/3197 [00:00<00:12, 232.73it/s]\u001b[A\n",
            "  7%|▋         | 233/3197 [00:01<00:12, 230.10it/s]\u001b[A\n",
            "  8%|▊         | 256/3197 [00:01<00:12, 229.47it/s]\u001b[A\n",
            "  9%|▉         | 281/3197 [00:01<00:12, 235.18it/s]\u001b[A\n",
            " 10%|▉         | 305/3197 [00:01<00:12, 229.75it/s]\u001b[A\n",
            " 10%|█         | 328/3197 [00:01<00:12, 227.66it/s]\u001b[A\n",
            " 11%|█         | 351/3197 [00:01<00:13, 208.44it/s]\u001b[A\n",
            " 12%|█▏        | 373/3197 [00:01<00:13, 203.38it/s]\u001b[A\n",
            " 12%|█▏        | 394/3197 [00:01<00:13, 200.37it/s]\u001b[A\n",
            " 13%|█▎        | 415/3197 [00:01<00:13, 198.85it/s]\u001b[A\n",
            " 14%|█▎        | 439/3197 [00:02<00:13, 207.07it/s]\u001b[A\n",
            " 15%|█▍        | 465/3197 [00:02<00:12, 218.86it/s]\u001b[A\n",
            " 15%|█▌        | 493/3197 [00:02<00:11, 233.02it/s]\u001b[A\n",
            " 16%|█▌        | 517/3197 [00:02<00:12, 221.86it/s]\u001b[A\n",
            " 17%|█▋        | 540/3197 [00:02<00:12, 208.50it/s]\u001b[A\n",
            " 18%|█▊        | 562/3197 [00:02<00:12, 208.13it/s]\u001b[A\n",
            " 18%|█▊        | 584/3197 [00:02<00:12, 204.23it/s]\u001b[A\n",
            " 19%|█▉        | 612/3197 [00:02<00:11, 220.68it/s]\u001b[A\n",
            " 20%|█▉        | 635/3197 [00:02<00:12, 213.19it/s]\u001b[A\n",
            " 21%|██        | 660/3197 [00:03<00:11, 221.89it/s]\u001b[A\n",
            " 21%|██▏       | 683/3197 [00:03<00:12, 207.17it/s]\u001b[A\n",
            " 22%|██▏       | 711/3197 [00:03<00:11, 224.22it/s]\u001b[A\n",
            " 23%|██▎       | 735/3197 [00:03<00:10, 227.24it/s]\u001b[A\n",
            " 24%|██▎       | 759/3197 [00:03<00:11, 221.25it/s]\u001b[A\n",
            " 24%|██▍       | 782/3197 [00:03<00:11, 213.40it/s]\u001b[A\n",
            " 25%|██▌       | 807/3197 [00:03<00:10, 220.88it/s]\u001b[A\n",
            " 26%|██▌       | 830/3197 [00:03<00:10, 215.79it/s]\u001b[A\n",
            " 27%|██▋       | 852/3197 [00:03<00:11, 203.48it/s]\u001b[A\n",
            " 27%|██▋       | 873/3197 [00:04<00:11, 203.31it/s]\u001b[A\n",
            " 28%|██▊       | 894/3197 [00:04<00:12, 189.20it/s]\u001b[A\n",
            " 29%|██▊       | 914/3197 [00:04<00:12, 180.36it/s]\u001b[A\n",
            " 29%|██▉       | 933/3197 [00:04<00:12, 177.98it/s]\u001b[A\n",
            " 30%|██▉       | 956/3197 [00:04<00:11, 189.67it/s]\u001b[A\n",
            " 31%|███       | 981/3197 [00:04<00:10, 203.82it/s]\u001b[A\n",
            " 31%|███▏      | 1007/3197 [00:04<00:10, 216.45it/s]\u001b[A\n",
            " 32%|███▏      | 1030/3197 [00:04<00:10, 203.36it/s]\u001b[A\n",
            " 33%|███▎      | 1053/3197 [00:04<00:10, 208.67it/s]\u001b[A\n",
            " 34%|███▍      | 1079/3197 [00:05<00:09, 220.87it/s]\u001b[A\n",
            " 35%|███▍      | 1104/3197 [00:05<00:09, 228.18it/s]\u001b[A\n",
            " 35%|███▌      | 1128/3197 [00:05<00:09, 216.75it/s]\u001b[A\n",
            " 36%|███▌      | 1151/3197 [00:05<00:09, 214.82it/s]\u001b[A\n",
            " 37%|███▋      | 1176/3197 [00:05<00:09, 223.62it/s]\u001b[A\n",
            " 38%|███▊      | 1199/3197 [00:05<00:09, 218.92it/s]\u001b[A\n",
            " 38%|███▊      | 1222/3197 [00:05<00:09, 214.43it/s]\u001b[A\n",
            " 39%|███▉      | 1244/3197 [00:05<00:09, 204.54it/s]\u001b[A\n",
            " 40%|███▉      | 1265/3197 [00:05<00:09, 201.19it/s]\u001b[A\n",
            " 40%|████      | 1286/3197 [00:06<00:09, 195.94it/s]\u001b[A\n",
            " 41%|████      | 1307/3197 [00:06<00:09, 198.50it/s]\u001b[A\n",
            " 42%|████▏     | 1329/3197 [00:06<00:09, 204.12it/s]\u001b[A\n",
            " 42%|████▏     | 1350/3197 [00:06<00:09, 205.02it/s]\u001b[A\n",
            " 43%|████▎     | 1371/3197 [00:06<00:09, 195.06it/s]\u001b[A\n",
            " 44%|████▎     | 1391/3197 [00:06<00:09, 184.12it/s]\u001b[A\n",
            " 44%|████▍     | 1414/3197 [00:06<00:09, 193.86it/s]\u001b[A\n",
            " 45%|████▍     | 1438/3197 [00:06<00:08, 205.04it/s]\u001b[A\n",
            " 46%|████▌     | 1463/3197 [00:06<00:08, 214.66it/s]\u001b[A\n",
            " 46%|████▋     | 1485/3197 [00:06<00:07, 214.24it/s]\u001b[A\n",
            " 47%|████▋     | 1508/3197 [00:07<00:07, 215.86it/s]\u001b[A\n",
            " 48%|████▊     | 1535/3197 [00:07<00:07, 229.35it/s]\u001b[A\n",
            " 49%|████▉     | 1560/3197 [00:07<00:07, 232.24it/s]\u001b[A\n",
            " 50%|████▉     | 1584/3197 [00:07<00:07, 227.25it/s]\u001b[A\n",
            " 50%|█████     | 1607/3197 [00:07<00:07, 211.99it/s]\u001b[A\n",
            " 51%|█████     | 1631/3197 [00:07<00:07, 218.79it/s]\u001b[A\n",
            " 52%|█████▏    | 1654/3197 [00:07<00:07, 205.41it/s]\u001b[A\n",
            " 52%|█████▏    | 1675/3197 [00:07<00:07, 203.80it/s]\u001b[A\n",
            " 53%|█████▎    | 1696/3197 [00:07<00:07, 195.34it/s]\u001b[A\n",
            " 54%|█████▎    | 1717/3197 [00:08<00:07, 198.00it/s]\u001b[A\n",
            " 54%|█████▍    | 1741/3197 [00:08<00:07, 206.43it/s]\u001b[A\n",
            " 55%|█████▌    | 1764/3197 [00:08<00:06, 212.84it/s]\u001b[A\n",
            " 56%|█████▌    | 1787/3197 [00:08<00:06, 214.97it/s]\u001b[A\n",
            " 57%|█████▋    | 1809/3197 [00:08<00:06, 211.12it/s]\u001b[A\n",
            " 57%|█████▋    | 1832/3197 [00:08<00:06, 216.42it/s]\u001b[A\n",
            " 58%|█████▊    | 1854/3197 [00:08<00:06, 211.65it/s]\u001b[A\n",
            " 59%|█████▉    | 1879/3197 [00:08<00:05, 221.72it/s]\u001b[A\n",
            " 59%|█████▉    | 1902/3197 [00:08<00:05, 218.18it/s]\u001b[A\n",
            " 60%|██████    | 1927/3197 [00:09<00:05, 225.00it/s]\u001b[A\n",
            " 61%|██████    | 1950/3197 [00:09<00:05, 224.42it/s]\u001b[A\n",
            " 62%|██████▏   | 1975/3197 [00:09<00:05, 230.68it/s]\u001b[A\n",
            " 63%|██████▎   | 1999/3197 [00:09<00:05, 216.97it/s]\u001b[A\n",
            " 63%|██████▎   | 2023/3197 [00:09<00:05, 223.02it/s]\u001b[A\n",
            " 64%|██████▍   | 2046/3197 [00:09<00:05, 219.14it/s]\u001b[A\n",
            " 65%|██████▍   | 2071/3197 [00:09<00:04, 225.99it/s]\u001b[A\n",
            " 66%|██████▌   | 2095/3197 [00:09<00:04, 228.42it/s]\u001b[A\n",
            " 66%|██████▋   | 2121/3197 [00:09<00:04, 235.02it/s]\u001b[A\n",
            " 67%|██████▋   | 2146/3197 [00:09<00:04, 237.46it/s]\u001b[A\n",
            " 68%|██████▊   | 2171/3197 [00:10<00:04, 237.39it/s]\u001b[A\n",
            " 69%|██████▊   | 2195/3197 [00:10<00:04, 237.45it/s]\u001b[A\n",
            " 69%|██████▉   | 2220/3197 [00:10<00:04, 240.55it/s]\u001b[A\n",
            " 70%|███████   | 2245/3197 [00:10<00:03, 240.26it/s]\u001b[A\n",
            " 71%|███████   | 2271/3197 [00:10<00:03, 245.81it/s]\u001b[A\n",
            " 72%|███████▏  | 2298/3197 [00:10<00:03, 251.15it/s]\u001b[A\n",
            " 73%|███████▎  | 2324/3197 [00:10<00:03, 219.60it/s]\u001b[A\n",
            " 73%|███████▎  | 2347/3197 [00:10<00:04, 209.46it/s]\u001b[A\n",
            " 74%|███████▍  | 2370/3197 [00:10<00:03, 214.00it/s]\u001b[A\n",
            " 75%|███████▍  | 2394/3197 [00:11<00:03, 217.89it/s]\u001b[A\n",
            " 76%|███████▌  | 2417/3197 [00:11<00:03, 212.73it/s]\u001b[A\n",
            " 76%|███████▋  | 2441/3197 [00:11<00:03, 218.57it/s]\u001b[A\n",
            " 77%|███████▋  | 2465/3197 [00:11<00:03, 223.70it/s]\u001b[A\n",
            " 78%|███████▊  | 2496/3197 [00:11<00:02, 243.17it/s]\u001b[A\n",
            " 79%|███████▉  | 2521/3197 [00:11<00:02, 229.40it/s]\u001b[A\n",
            " 80%|███████▉  | 2545/3197 [00:11<00:03, 213.14it/s]\u001b[A\n",
            " 80%|████████  | 2567/3197 [00:11<00:03, 206.26it/s]\u001b[A\n",
            " 81%|████████  | 2589/3197 [00:11<00:03, 201.73it/s]\u001b[A\n",
            " 82%|████████▏ | 2610/3197 [00:12<00:02, 199.69it/s]\u001b[A\n",
            " 82%|████████▏ | 2631/3197 [00:12<00:02, 192.96it/s]\u001b[A\n",
            " 83%|████████▎ | 2657/3197 [00:12<00:02, 209.12it/s]\u001b[A\n",
            " 84%|████████▍ | 2683/3197 [00:12<00:02, 220.13it/s]\u001b[A\n",
            " 85%|████████▍ | 2706/3197 [00:12<00:02, 219.06it/s]\u001b[A\n",
            " 85%|████████▌ | 2729/3197 [00:12<00:02, 217.89it/s]\u001b[A\n",
            " 86%|████████▋ | 2760/3197 [00:12<00:01, 238.89it/s]\u001b[A\n",
            " 87%|████████▋ | 2788/3197 [00:12<00:01, 247.65it/s]\u001b[A\n",
            " 88%|████████▊ | 2814/3197 [00:12<00:01, 245.80it/s]\u001b[A\n",
            " 89%|████████▉ | 2842/3197 [00:13<00:01, 254.51it/s]\u001b[A\n",
            " 90%|█████████ | 2879/3197 [00:13<00:01, 280.01it/s]\u001b[A\n",
            " 91%|█████████ | 2909/3197 [00:13<00:01, 280.43it/s]\u001b[A\n",
            " 92%|█████████▏| 2938/3197 [00:13<00:00, 267.61it/s]\u001b[A\n",
            " 93%|█████████▎| 2966/3197 [00:13<00:00, 264.93it/s]\u001b[A\n",
            " 94%|█████████▍| 2998/3197 [00:13<00:00, 278.04it/s]\u001b[A\n",
            " 95%|█████████▍| 3027/3197 [00:13<00:00, 244.92it/s]\u001b[A\n",
            " 95%|█████████▌| 3053/3197 [00:13<00:00, 241.54it/s]\u001b[A\n",
            " 96%|█████████▋| 3078/3197 [00:13<00:00, 225.90it/s]\u001b[A\n",
            " 97%|█████████▋| 3102/3197 [00:14<00:00, 219.05it/s]\u001b[A\n",
            " 98%|█████████▊| 3125/3197 [00:14<00:00, 213.68it/s]\u001b[A\n",
            " 99%|█████████▊| 3150/3197 [00:14<00:00, 223.29it/s]\u001b[A\n",
            " 99%|█████████▉| 3176/3197 [00:14<00:00, 231.16it/s]\u001b[A\n",
            "100%|██████████| 3197/3197 [00:14<00:00, 220.75it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "second prediction vlayes\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Confusion matrix, without normalization\n",
            "[[1323  344]\n",
            " [1241  289]]\n",
            "training loss/accuracy:  0.543166399313056 / 0.755\n",
            "validation loss/accuracy:  0.5754825764834278 / 0.7263257575757576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAGACAYAAADS/Qc0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUfCJiIhdUfCJiIhdUfCJiIhd+X9PHDOZLzettwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ljRpA2shuet1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ensemble Model\n",
        "### MobileNetV2 + DenseNet + Resnet50 (pretrained + downloaded from Dropbox)"
      ]
    },
    {
      "metadata": {
        "id": "eB7Q0lcYuXQz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# all_models = list()\n",
        "# for i in listdir(dir):\n",
        " \n",
        "#   if i == 'model_NASNetMobile.h5' or i == 'model_ensemble.h5':\n",
        "#     continue\n",
        "    \n",
        "#   filename = dir + i\n",
        "#   # load model from file\n",
        "#   model = load_model(filename)\n",
        "#   # add to list of members\n",
        "#   all_models.append(model)\n",
        "#   print('loaded ',filename)\n",
        "\n",
        "# def ensemble_model(all_models):\n",
        "  \n",
        "#   for i in range(len(all_models)):\n",
        "#     model = all_models[i]\n",
        "#     for layer in model.layers:\n",
        "#       # make not trainable\n",
        "#       layer.trainable = False\n",
        "#       # rename to avoid 'unique layer name' issue\n",
        "#       layer.name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
        "#   # define multi-headed input\n",
        "#   ensemble_visible = [model.input for model in all_models]\n",
        "#   # concatenate merge output from each model\n",
        "#   ensemble_outputs = [model.output for model in all_models]\n",
        "#   merge = concatenate(ensemble_outputs)\n",
        "#   #print(merge.shape)\n",
        "#   #merge = keras.layers.Permute((2,1))(merge)\n",
        "#   #merge=keras.layers.Reshape((3,),dtype=tf.float32_ref)(merge)\n",
        "#   hidden = Dense(128, activation='relu')(merge)\n",
        "#   output = Dense(1, activation='sigmoid')(hidden)\n",
        "#   model = Model(inputs=ensemble_visible, outputs=output)\n",
        "  \n",
        "#   # plot graph of ensemble\n",
        "#   plot_model(model, show_shapes=True, to_file='ensemble_model_graph.png')\n",
        "#   # compile\n",
        "#   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#   return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zj5DONfhEhmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @timer\n",
        "# def eval_ensemble(model_file='model_ensemble.h5',epoch=3,batch=64,verbose=2):\n",
        "  \n",
        "  \n",
        "#   df_train=pd.read_csv('train_paths_labels.csv')\n",
        "#   df_valid=pd.read_csv('valid_paths_labels.csv')\n",
        "  \n",
        "#   datagen=ImageDataGenerator(rescale=1./255)\n",
        "#   train_generator=datagen.flow_from_dataframe(dataframe=df_train, directory=None,x_col=\"Img_Path\", y_col=\"Label\", class_mode=\"binary\",\n",
        "#                                               target_size=(224,224), batch_size=batch)\n",
        "#   valid_generator=datagen.flow_from_dataframe(dataframe=df_valid, directory=None,x_col=\"Img_Path\", y_col=\"Label\", class_mode=\"binary\",\n",
        "#                                               target_size=(224,224), batch_size=batch)\n",
        "  \n",
        "#   keras.backend.clear_session()\n",
        "  \n",
        "  \n",
        "#   if not os.path.isfile(dir + model_file):\n",
        "#     print(\"making ensemble model\")\n",
        "#     model = ensemble_model(all_models)\n",
        "#   else:\n",
        "#     print(\"loading saved ensemble model\")\n",
        "#     model= load_model(dir + model_file)\n",
        "  \n",
        "#   print(\"compiling\")\n",
        "#   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "#   checkpoint= ModelCheckpoint(dir + model_file, monitor='val_loss', verbose=1, save_best_only=False,\n",
        "#                               save_weights_only=False, mode='auto', period=1)\n",
        "#   callbacks_list = [checkpoint]\n",
        "    \n",
        "#   step_train=train_generator.n//train_generator.batch_size\n",
        "#   step_valid=valid_generator.n//valid_generator.batch_size\n",
        "  \n",
        "#   model.fit_generator(generator=train_generator, steps_per_epoch=step_train, epochs=epoch, validation_data=valid_generator,\n",
        "#                       validation_steps=step_valid, shuffle=True, verbose=verbose, callbacks=callbacks_list)\n",
        "  \n",
        "  \n",
        "  \n",
        "#   loss_tr, accuracy_tr = model.evaluate_generator(train_generator, use_multiprocessing=True,workers=5,steps=step_train)\n",
        "#   print(\"training loss/accuracy: \", loss_tr,'/', accuracy_tr)\n",
        "\n",
        "#   loss_val, accuracy_val = model.evaluate_generator(valid_generator, use_multiprocessing=True,workers=5,steps=step_valid)\n",
        "#   print(\"validation loss/accuracy: \", loss_val,'/', accuracy_val)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4nYYbTcHGI42",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #@title SubModel Evaluation { form-width: \"30%\" }\n",
        "# epoch = 3 #@param {type:\"integer\"}\n",
        "# batch_size = 64 #@param {type:\"integer\"}\n",
        "\n",
        "# model_file= 'model_ensemble.h5'\n",
        "\n",
        "# p = mp.Process(target=watch_, args=(model_file,20))\n",
        "# p.start()\n",
        "# eval_ensemble(model_file=model_file,epoch=3,batch=batch_size,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6h24yRz4_rts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}