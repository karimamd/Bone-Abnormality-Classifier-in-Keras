{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MURA Clean Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Corv113DVLKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ]
    },
    {
      "metadata": {
        "id": "SmaRONjkdPJc",
        "colab_type": "code",
        "outputId": "376a80ef-2c63-48a9-f553-c946965d5eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cfvs-0PJVXGi",
        "colab_type": "code",
        "outputId": "a91b45c5-9526-4cb2-f121-15a8bcc7f966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "#!wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "#!unzip MURA-v1.1.zip\n",
        "#!rm MURA-v1.1.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-26 06:43:31--  https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3380245855 (3.1G) [application/zip]\n",
            "Saving to: ‘MURA-v1.1.zip’\n",
            "\n",
            "MURA-v1.1.zip        35%[======>             ]   1.12G  14.4MB/s    eta 2m 33s "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hezUpQAcVLK6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7499467f-2998-4c1a-d49f-ca1b66b6cdc8"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "pd.options.display.max_colwidth = 100\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ut2PF0_mOa_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Reading**"
      ]
    },
    {
      "metadata": {
        "id": "dBWtYEWrOeOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paths_n_labels(csv,str_limp):\n",
        "    #make dataframe\n",
        "    studies=pd.read_csv(csv, sep=',',header=None)\n",
        "    #separate study paths and labels of given limp from those of other limps\n",
        "    limp_studies=studies[studies[0].str.contains(str_limp)==True]\n",
        "    #make it a numpy\n",
        "    limp_studies=np.array(limp_studies)\n",
        "    #limp study folder paths\n",
        "    limp_paths=[]\n",
        "    #labels of given limp\n",
        "    limp_labels=[]\n",
        "    for i in tqdm( range(limp_studies.shape[0]) ):\n",
        "        study_path=limp_studies[i][0]\n",
        "        study_label=limp_studies[i][1]\n",
        "        study_files = [f for f in listdir(study_path) if isfile(join(study_path, f))]\n",
        "        for image in study_files:\n",
        "            limp_paths.append(study_path + image)\n",
        "            limp_labels.append(study_label)\n",
        "\n",
        "    limp_paths=np.array(limp_paths)\n",
        "    limp_labels=np.array(limp_labels)\n",
        "\n",
        "    return limp_paths,limp_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d-KrxmIReoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#general function with options for wrist data case, only set wrist_train=True in the case of wrist training data only\n",
        "#for all other limps and for validation data even that of wrist just pass the paths\n",
        "#targ_size is image resizing with default(224,224)\n",
        "#preprocess flag is for using keras preprocessing for images or just resizing\n",
        "def read_images(paths ,targ_size= (224, 224), wrist_train=False, preprocess=False):\n",
        "    images=[]\n",
        "    #load any limp images\n",
        "    if(not wrist_train):\n",
        "        for path in tqdm(paths):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size )\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    #special case for wrist train corrupted data       \n",
        "    else:\n",
        "        #did this because it gave an error at sample  5307 or near it if took all\n",
        "        sample_e=5307\n",
        "        sample_s2=5339\n",
        "        images=[]\n",
        "        for path in tqdm(paths[:sample_e]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "        #new start\n",
        "        for path in tqdm(paths[sample_s2:]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "    #making it a numpy array instead of python list\n",
        "    return (np.array(images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9xFCfDa2aR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_studies='MURA-v1.1/train_labeled_studies.csv'\n",
        "valid_studies='MURA-v1.1/valid_labeled_studies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4QggWXq2iu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e27d7c75-53c9-4aba-ddc0-e03895fcdd82"
      },
      "cell_type": "code",
      "source": [
        "#training images paths and labels\n",
        "#starting with wrist data\n",
        "train_paths,train_labels=paths_n_labels(train_studies,\"WRIST\")\n",
        "#data bias : train\n",
        "print(\"0 normal, 1 abnormal\")\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3460/3460 [00:02<00:00, 1417.34it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 normal, 1 abnormal\n",
            "{0: 5769, 1: 3987}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "58kzFDVzGzzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cc96bd22-8b09-4970-b9b1-4ff7a29fb331"
      },
      "cell_type": "code",
      "source": [
        "#comment this if not wrist train data\n",
        "def wrist_labels(labels):\n",
        "  sample_e=5307\n",
        "  sample_s2=5339\n",
        "  return np.hstack( [ labels[:sample_e], labels[sample_s2:] ])\n",
        "\n",
        "print(train_labels.shape)\n",
        "w_labels=wrist_labels(train_labels)\n",
        "print(w_labels.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9756,)\n",
            "(9724,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vJ_NG3xJOjfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c5b55857-5182-4798-8962-ce3845c02552"
      },
      "cell_type": "code",
      "source": [
        "#validation images paths and labels\n",
        "#starting with wrist data\n",
        "valid_paths,valid_labels=paths_n_labels(valid_studies,\"WRIST\")\n",
        "#data bias : valid\n",
        "print(\"0 normal, 1 abnormal\")\n",
        "unique, counts = np.unique(valid_labels, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 237/237 [00:00<00:00, 1543.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 normal, 1 abnormal\n",
            "{0: 364, 1: 295}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6eKYDPMqsEmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #sample before preprocessing\n",
        "# plt.imshow( (train_imgs[10]+1)/2 )\n",
        "# print(\"maximum and min values before and after rescaling for matplotlip\")\n",
        "# print(np.max(train_imgs[10]),np.min(train_imgs[10]), np.max((train_imgs[10]+1)/2), np.min((train_imgs[10]+1)/2) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1u2CNQ7VLMC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "mMST4D272Whd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=True):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetLarge(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "    \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "  if(add_denses):\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    # and a logistic layer -- let's say we have 200 classes\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "  else:\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MiQRBhNFRfqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Read images to memory and preprocessing**"
      ]
    },
    {
      "metadata": {
        "id": "-7cjQg8P3y2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e682ccbb-8c49-4740-b71e-552c6d72df42"
      },
      "cell_type": "code",
      "source": [
        "# preprocess input\n",
        "train_imgs=read_images(train_paths,preprocess=True, wrist_train=True)\n",
        "valid_imgs=read_images(valid_paths,preprocess=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5307/5307 [00:35<00:00, 149.42it/s]\n",
            "100%|██████████| 4417/4417 [00:25<00:00, 175.50it/s]\n",
            "100%|██████████| 659/659 [00:04<00:00, 153.30it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "iKKcEId-RnWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "142edd7d-e436-45d5-acf0-65828ddd8a66"
      },
      "cell_type": "code",
      "source": [
        "print(train_imgs.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9724, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7EJIFlNW6-SA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "7750c995-1548-4f03-9cc0-ae0eab3cc417"
      },
      "cell_type": "code",
      "source": [
        "model=make_FT_model(base=2, imagenet=False, freeze_all=False, add_denses=True)\n",
        "#m.summary()\n",
        "#Choose adam or RMSprop ?!\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "'''\n",
        "IF USING OTHER DATA THAN WRIST CHANGE w_labels to train_labels\n",
        "'''\n",
        "model.fit(train_imgs, w_labels, epochs=5, validation_data=(valid_imgs, valid_labels), shuffle=True, verbose=1 )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 9724 samples, validate on 659 samples\n",
            "Epoch 1/5\n",
            "9724/9724 [==============================] - 239s 25ms/step - loss: 0.7025 - acc: 0.5809 - val_loss: 4.9843 - val_acc: 0.5524\n",
            "Epoch 2/5\n",
            "9724/9724 [==============================] - 204s 21ms/step - loss: 0.6749 - acc: 0.5916 - val_loss: 6.6514 - val_acc: 0.5554\n",
            "Epoch 3/5\n",
            "9724/9724 [==============================] - 205s 21ms/step - loss: 0.6688 - acc: 0.5884 - val_loss: 0.6957 - val_acc: 0.5524\n",
            "Epoch 4/5\n",
            "9724/9724 [==============================] - 204s 21ms/step - loss: 0.6566 - acc: 0.5912 - val_loss: 1.8280 - val_acc: 0.5478\n",
            "Epoch 5/5\n",
            "9724/9724 [==============================] - 205s 21ms/step - loss: 0.6543 - acc: 0.6090 - val_loss: 1.7099 - val_acc: 0.5690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5311b40908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "5EM1-Kd8-lCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e4a0531f-e3c8-4a69-e3d1-88f2a6625628"
      },
      "cell_type": "code",
      "source": [
        "#removed loss from printing because idk its use and intuition\n",
        "loss,accuracy=model.evaluate(x=train_imgs, y=w_labels, batch_size=128, verbose=1)\n",
        "print(\"train accuracy\",accuracy)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9724/9724 [==============================] - 50s 5ms/step\n",
            "train accuracy 0.5866927186778816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3VBhd8q0-qmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "caa8c531-82e3-4f8b-aa3f-e55a9c173bc4"
      },
      "cell_type": "code",
      "source": [
        "loss,accuracy=model.evaluate(x=valid_imgs, y=valid_labels, batch_size=128, verbose=1)\n",
        "print(\"validation accuracy\",accuracy)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "659/659 [==============================] - 3s 5ms/step\n",
            "validation accuracy 0.5690440067029325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0_9VVa8XVLMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Current point: generalizing functions and code cleaning+ seeing early stopping callback**"
      ]
    },
    {
      "metadata": {
        "id": "vt5njgtKmMOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All unsolved problems:\n",
        "\n",
        "* which layers to freeze and which to train + should I train TL before freezing it ?\n",
        "* data augmentation to generate more data (solved by khaled,still needs little verification)\n",
        "* recording variation in accuracy after every change to get intuition\n",
        "* what does outputted loss represent ? how to read the number ?\n",
        "* normalization step and its effect on accuracy\n",
        "* should I use 1 or two neurons at output layer ?\n",
        "* binary crossentropy weights\n",
        "* justifying parameter use and discovering useful params\n",
        "* try training with model unfrozen with imagenet and without it\n",
        "* make a function to record and tabulate outputs\n",
        "* could we add precision or recall metric ? change accuracy?\n",
        "* why doesn't it work if removed GlobalAveragePooling line?\n",
        "* see if want to freeze less layers\n",
        "* generalize file reading functions\n",
        "* use better batch size ? increase epochs ?\n",
        "* training function with preprocessing flag and multiple model comparisons\n",
        "* grid search like function to tune models and hyperparameters\n",
        "* early stopping callback and save model to drive for continuing training\n",
        "* feature concatenation"
      ]
    },
    {
      "metadata": {
        "id": "IVMVHTzRF7f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**results on wrist data: no preprocessing but resize**\n",
        "\n",
        "\n",
        "1- Densenet, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "2- Densenet with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "65% train and 65% validation\n",
        "\n",
        "3-InceptionV3, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "4-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "82% train and 78% validation\n",
        "\n",
        "\n",
        "\n",
        "**results on wrist data: with keras preprocessing **\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them)\n",
        "71% train and 68% validation\n",
        "\n",
        "epoch=6 mins\n",
        "\n",
        "2- Densenet WITHOUT imagenet and didnt freeze \n",
        "\n",
        "62% train and 59% validation\n",
        "\n",
        "epoch=5.5 mins\n",
        "\n",
        "3-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "79% train and 74% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "4-InceptionV3 WITHOUT imagenet and didnt freeze \n",
        "59% train and 57% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "** Conclusions till now **\n",
        "\n",
        "1- keras preprocessing (nomalization and subtracting mean maybe ) with inception reduces  accuracy both training and validation , but with DenseNet it improved accuracy \n",
        "\n",
        "2- Training over imagenet weights without freezing the TL model gives significantly higher accuracy than freezing whole TL and just training denses\n",
        "\n",
        "3-Training TL without imagenet and no freeze till now (2 Trials) seems to get worse results than with imagenet and training over it"
      ]
    },
    {
      "metadata": {
        "id": "ghAljjlsYGgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The  Generator**"
      ]
    },
    {
      "metadata": {
        "id": "YX4l8VYbVLM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(featurewise_center=True,\n",
        "                     featurewise_std_normalization=True,\n",
        "                     rotation_range=90,\n",
        "                     width_shift_range=0.1,\n",
        "                     height_shift_range=0.1,\n",
        "                     zoom_range=0.2)\n",
        "\n",
        "# compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied)\n",
        "# datagen.fit(wrist_images_tr)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "# model.fit_generator(datagen.flow(x_train, wrist_labels_tr, batch_size=32),\n",
        "#                     steps_per_epoch=len(wrist_images_tr) / 32, epochs=epochs)\n",
        "\n",
        "\n",
        "print(train_imgs.shape)\n",
        "print(w_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mh2fOYcTYUOY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=make_FT_model(base=1, imagenet=True, freeze_all=False, add_denses=True)\n",
        "#m.summary()\n",
        "#Choose adam or RMSprop ?!\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(train_imgs, w_labels, batch_size=16),\n",
        "                    steps_per_epoch=len(train_imgs) / 16, epochs=10,use_multiprocessing=True,workers=6)\n",
        "\n",
        "# model.fit(wrist_images_tr, wrist_labels_tr, epochs=5, validation_data=(wrist_images_val, wrist_labels_val), shuffle=True, verbose=1 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_PQ7bDJYXym",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#removed loss from printing because idk its use and intuition\n",
        "loss,accuracy=model.evaluate(x=train_imgs, y=w_labels, batch_size=128, verbose=1)\n",
        "print(\"train accuracy\",accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YPb-rYmzYYtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss,accuracy=model.evaluate(x=valid_imgs, y=valid_labels, batch_size=128, verbose=1)\n",
        "print(\"validation accuracy\",accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uR-g2EsQcnmm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generalizing over all limps"
      ]
    },
    {
      "metadata": {
        "id": "R1VohqXoheEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def images_n_labels(limp):\n",
        "  train_paths,train_labels=paths_n_labels(train_studies,limp)\n",
        "  valid_paths,valid_labels=paths_n_labels(valid_studies,limp)\n",
        "  \n",
        "  if (limp == \"WRIST\"):\n",
        "    train_labels=wrist_labels(train_labels)\n",
        "    train_imgs= read_images(train_paths,preprocess=True, wrist_train=True)\n",
        "  \n",
        "  else:\n",
        "    train_imgs= read_images(train_paths,preprocess=True, wrist_train=False)\n",
        "    \n",
        "  valid_imgs= read_images(valid_paths,preprocess=True)\n",
        "  \n",
        "  return train_imgs, train_labels, valid_imgs, valid_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eXYxHrw_cqIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limps=[\"SOULDER\", \"WRIST\", \"FINGER\", \"ELBOW\", \"HUMERUS\", \"HAND\", \"FOREARM\"]\n",
        "\n",
        "\n",
        "#pass to model list of limps because if wanted to train on less\n",
        "#function outputs a dictionary or dataframe has train and val accuracies for each limp using a chosen model\n",
        "\n",
        "def evaluate_limps(model_no=2, imagenet=True, freeze_all=False,v=1 , limps=limps):\n",
        "  accuracies={}\n",
        "  for limp in limps:\n",
        "    train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp)\n",
        "    model=make_FT_model(base= model_no, imagenet=False, freeze_all=False, add_denses=True)\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_imgs, train_labels, epochs=5, validation_data=(valid_imgs, valid_labels), shuffle=True, verbose=v )\n",
        "    loss_tr, accuracy_tr =model.evaluate(x=train_imgs, y=train_labels, batch_size=128, verbose=v)\n",
        "    loss_val, accuracy_val =model.evaluate(x=valid_imgs, y=valid_labels, batch_size=128, verbose=1)\n",
        "    accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "  \n",
        "  return accuracies\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxKTrlBCt5S9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5192
        },
        "outputId": "ba5d6ac8-87dc-43b9-96f0-30cb51293751"
      },
      "cell_type": "code",
      "source": [
        "W=evaluate_limps(limps=['WRIST'])\n",
        "print(W.values())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3460 [00:00<?, ?it/s]\u001b[A\n",
            " 50%|█████     | 1747/3460 [00:00<00:00, 17469.68it/s]\u001b[A\n",
            " 93%|█████████▎| 3220/3460 [00:00<00:00, 16543.45it/s]\u001b[A\n",
            "100%|██████████| 3460/3460 [00:00<00:00, 14571.25it/s]\u001b[A\n",
            "  0%|          | 0/237 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 237/237 [00:00<00:00, 7824.02it/s]\u001b[A\n",
            "  0%|          | 0/5307 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 3/5307 [00:00<03:11, 27.67it/s]\u001b[A\n",
            "  0%|          | 5/5307 [00:00<04:10, 21.20it/s]\u001b[A\n",
            "  0%|          | 7/5307 [00:00<04:19, 20.43it/s]\u001b[A\n",
            "  0%|          | 10/5307 [00:00<04:14, 20.84it/s]\u001b[A\n",
            "  0%|          | 12/5307 [00:00<04:44, 18.63it/s]\u001b[A\n",
            "  0%|          | 14/5307 [00:00<05:10, 17.06it/s]\u001b[A\n",
            "  0%|          | 16/5307 [00:00<05:29, 16.08it/s]\u001b[A\n",
            "  0%|          | 18/5307 [00:01<05:34, 15.80it/s]\u001b[A\n",
            "  0%|          | 20/5307 [00:01<05:16, 16.72it/s]\u001b[A\n",
            "  0%|          | 22/5307 [00:01<05:11, 16.96it/s]\u001b[A\n",
            "  0%|          | 24/5307 [00:01<04:59, 17.65it/s]\u001b[A\n",
            "  0%|          | 26/5307 [00:01<04:54, 17.91it/s]\u001b[A\n",
            "  1%|          | 29/5307 [00:01<04:37, 19.04it/s]\u001b[A\n",
            "  1%|          | 31/5307 [00:01<04:58, 17.70it/s]\u001b[A\n",
            "  1%|          | 33/5307 [00:01<05:14, 16.78it/s]\u001b[A\n",
            "  1%|          | 35/5307 [00:02<05:24, 16.26it/s]\u001b[A\n",
            "  1%|          | 37/5307 [00:02<05:31, 15.89it/s]\u001b[A\n",
            "  1%|          | 39/5307 [00:02<05:51, 15.01it/s]\u001b[A\n",
            "  1%|          | 42/5307 [00:02<05:15, 16.71it/s]\u001b[A\n",
            "  1%|          | 45/5307 [00:02<04:35, 19.09it/s]\u001b[A\n",
            "  1%|          | 49/5307 [00:02<04:04, 21.52it/s]\u001b[A\n",
            "  1%|          | 52/5307 [00:02<04:09, 21.07it/s]\u001b[A\n",
            "  1%|          | 55/5307 [00:02<04:00, 21.87it/s]\u001b[A\n",
            "  1%|          | 58/5307 [00:03<04:02, 21.64it/s]\u001b[A\n",
            "  1%|          | 61/5307 [00:03<03:54, 22.37it/s]\u001b[A\n",
            "  1%|          | 64/5307 [00:03<04:05, 21.40it/s]\u001b[A\n",
            "  1%|▏         | 67/5307 [00:03<03:46, 23.17it/s]\u001b[A\n",
            "  1%|▏         | 70/5307 [00:03<03:46, 23.16it/s]\u001b[A\n",
            "  1%|▏         | 73/5307 [00:03<03:46, 23.09it/s]\u001b[A\n",
            "  1%|▏         | 76/5307 [00:03<03:43, 23.38it/s]\u001b[A\n",
            "  1%|▏         | 79/5307 [00:03<03:30, 24.89it/s]\u001b[A\n",
            "  2%|▏         | 82/5307 [00:04<03:42, 23.53it/s]\u001b[A\n",
            "  2%|▏         | 85/5307 [00:04<03:50, 22.63it/s]\u001b[A\n",
            "  2%|▏         | 88/5307 [00:04<03:48, 22.84it/s]\u001b[A\n",
            "  2%|▏         | 91/5307 [00:04<03:33, 24.47it/s]\u001b[A\n",
            "  2%|▏         | 94/5307 [00:04<03:37, 24.01it/s]\u001b[A\n",
            "  2%|▏         | 97/5307 [00:04<03:48, 22.82it/s]\u001b[A\n",
            "  2%|▏         | 100/5307 [00:04<03:54, 22.16it/s]\u001b[A\n",
            "  2%|▏         | 103/5307 [00:05<04:05, 21.21it/s]\u001b[A\n",
            "  2%|▏         | 106/5307 [00:05<04:11, 20.68it/s]\u001b[A\n",
            "  2%|▏         | 109/5307 [00:05<04:22, 19.81it/s]\u001b[A\n",
            "  2%|▏         | 112/5307 [00:05<03:58, 21.76it/s]\u001b[A\n",
            "  2%|▏         | 115/5307 [00:05<03:55, 22.02it/s]\u001b[A\n",
            "  2%|▏         | 118/5307 [00:05<03:52, 22.33it/s]\u001b[A\n",
            "  2%|▏         | 121/5307 [00:05<03:58, 21.73it/s]\u001b[A\n",
            "  2%|▏         | 124/5307 [00:06<03:59, 21.62it/s]\u001b[A\n",
            "  2%|▏         | 127/5307 [00:06<04:06, 21.04it/s]\u001b[A\n",
            "  2%|▏         | 130/5307 [00:06<04:14, 20.33it/s]\u001b[A\n",
            "  3%|▎         | 133/5307 [00:06<04:24, 19.57it/s]\u001b[A\n",
            "  3%|▎         | 135/5307 [00:06<04:23, 19.60it/s]\u001b[A\n",
            "  3%|▎         | 137/5307 [00:06<04:22, 19.69it/s]\u001b[A\n",
            "  3%|▎         | 139/5307 [00:06<04:22, 19.72it/s]\u001b[A\n",
            "  3%|▎         | 141/5307 [00:06<04:24, 19.54it/s]\u001b[A\n",
            "  3%|▎         | 144/5307 [00:07<04:08, 20.81it/s]\u001b[A\n",
            "  3%|▎         | 147/5307 [00:07<04:06, 20.92it/s]\u001b[A\n",
            "  3%|▎         | 151/5307 [00:07<03:40, 23.37it/s]\u001b[A\n",
            "  3%|▎         | 154/5307 [00:07<04:00, 21.44it/s]\u001b[A\n",
            "  3%|▎         | 157/5307 [00:07<04:06, 20.87it/s]\u001b[A\n",
            "  3%|▎         | 160/5307 [00:07<03:44, 22.91it/s]\u001b[A\n",
            "  3%|▎         | 163/5307 [00:07<03:47, 22.66it/s]\u001b[A\n",
            "  3%|▎         | 166/5307 [00:07<03:58, 21.55it/s]\u001b[A\n",
            "  3%|▎         | 169/5307 [00:08<04:05, 20.93it/s]\u001b[A\n",
            "  3%|▎         | 172/5307 [00:08<04:07, 20.77it/s]\u001b[A\n",
            "  3%|▎         | 175/5307 [00:08<04:12, 20.36it/s]\u001b[A\n",
            "  3%|▎         | 178/5307 [00:08<04:11, 20.43it/s]\u001b[A\n",
            "  3%|▎         | 181/5307 [00:08<04:08, 20.64it/s]\u001b[A\n",
            "  3%|▎         | 184/5307 [00:08<04:10, 20.48it/s]\u001b[A\n",
            "  4%|▎         | 187/5307 [00:09<03:58, 21.50it/s]\u001b[A\n",
            "  4%|▎         | 190/5307 [00:09<04:01, 21.20it/s]\u001b[A\n",
            "  4%|▎         | 193/5307 [00:09<04:04, 20.88it/s]\u001b[A\n",
            "  4%|▎         | 196/5307 [00:09<04:08, 20.60it/s]\u001b[A\n",
            "  4%|▎         | 199/5307 [00:09<03:58, 21.38it/s]\u001b[A\n",
            "  4%|▍         | 202/5307 [00:09<03:39, 23.21it/s]\u001b[A\n",
            "  4%|▍         | 205/5307 [00:09<03:48, 22.31it/s]\u001b[A\n",
            "  4%|▍         | 208/5307 [00:09<03:58, 21.41it/s]\u001b[A\n",
            "  4%|▍         | 211/5307 [00:10<04:07, 20.61it/s]\u001b[A\n",
            "  4%|▍         | 214/5307 [00:10<04:11, 20.22it/s]\u001b[A\n",
            "  4%|▍         | 217/5307 [00:10<04:16, 19.82it/s]\u001b[A\n",
            "  4%|▍         | 220/5307 [00:10<04:21, 19.48it/s]\u001b[A\n",
            "  4%|▍         | 224/5307 [00:10<03:52, 21.88it/s]\u001b[A\n",
            "  4%|▍         | 227/5307 [00:10<03:36, 23.46it/s]\u001b[A\n",
            "  4%|▍         | 230/5307 [00:11<03:47, 22.36it/s]\u001b[A\n",
            "  4%|▍         | 233/5307 [00:11<03:31, 23.99it/s]\u001b[A\n",
            "  4%|▍         | 236/5307 [00:11<03:29, 24.22it/s]\u001b[A\n",
            "  5%|▍         | 239/5307 [00:11<03:40, 22.94it/s]\u001b[A\n",
            "  5%|▍         | 242/5307 [00:11<03:50, 21.97it/s]\u001b[A\n",
            "  5%|▍         | 245/5307 [00:11<03:58, 21.20it/s]\u001b[A\n",
            "  5%|▍         | 248/5307 [00:11<03:57, 21.33it/s]\u001b[A\n",
            "  5%|▍         | 251/5307 [00:11<03:57, 21.27it/s]\u001b[A\n",
            "  5%|▍         | 254/5307 [00:12<03:49, 21.99it/s]\u001b[A\n",
            "  5%|▍         | 258/5307 [00:12<03:26, 24.46it/s]\u001b[A\n",
            "  5%|▍         | 261/5307 [00:12<03:37, 23.18it/s]\u001b[A\n",
            "  5%|▍         | 264/5307 [00:12<03:43, 22.56it/s]\u001b[A\n",
            "  5%|▌         | 267/5307 [00:12<03:51, 21.77it/s]\u001b[A\n",
            "  5%|▌         | 271/5307 [00:12<03:34, 23.53it/s]\u001b[A\n",
            "  5%|▌         | 274/5307 [00:12<03:38, 23.04it/s]\u001b[A\n",
            "  5%|▌         | 277/5307 [00:13<03:31, 23.74it/s]\u001b[A\n",
            "  5%|▌         | 281/5307 [00:13<03:22, 24.88it/s]\u001b[A\n",
            "  5%|▌         | 285/5307 [00:13<02:59, 27.99it/s]\u001b[A\n",
            "  5%|▌         | 289/5307 [00:13<02:52, 29.15it/s]\u001b[A\n",
            "  6%|▌         | 293/5307 [00:13<02:53, 28.82it/s]\u001b[A\n",
            "  6%|▌         | 297/5307 [00:13<02:53, 28.92it/s]\u001b[A\n",
            "  6%|▌         | 300/5307 [00:13<03:15, 25.67it/s]\u001b[A\n",
            "  6%|▌         | 303/5307 [00:13<03:29, 23.86it/s]\u001b[A\n",
            "  6%|▌         | 306/5307 [00:14<03:38, 22.92it/s]\u001b[A\n",
            "  6%|▌         | 309/5307 [00:14<03:41, 22.60it/s]\u001b[A\n",
            "  6%|▌         | 312/5307 [00:14<03:43, 22.37it/s]\u001b[A\n",
            "  6%|▌         | 316/5307 [00:14<03:21, 24.80it/s]\u001b[A\n",
            "  6%|▌         | 319/5307 [00:14<03:35, 23.18it/s]\u001b[A\n",
            "  6%|▌         | 322/5307 [00:14<03:38, 22.86it/s]\u001b[A\n",
            "  6%|▌         | 326/5307 [00:14<03:18, 25.04it/s]\u001b[A\n",
            "  6%|▌         | 329/5307 [00:15<03:30, 23.64it/s]\u001b[A\n",
            "  6%|▋         | 332/5307 [00:15<03:41, 22.46it/s]\u001b[A\n",
            "  6%|▋         | 335/5307 [00:15<03:44, 22.18it/s]\u001b[A\n",
            "  6%|▋         | 338/5307 [00:15<03:45, 22.04it/s]\u001b[A\n",
            "  6%|▋         | 341/5307 [00:15<03:48, 21.71it/s]\u001b[A\n",
            "  6%|▋         | 344/5307 [00:15<03:51, 21.45it/s]\u001b[A\n",
            "  7%|▋         | 347/5307 [00:15<03:49, 21.59it/s]\u001b[A\n",
            "  7%|▋         | 350/5307 [00:16<03:38, 22.66it/s]\u001b[A\n",
            "  7%|▋         | 354/5307 [00:16<03:11, 25.83it/s]\u001b[A\n",
            "  7%|▋         | 357/5307 [00:16<03:15, 25.38it/s]\u001b[A\n",
            "  7%|▋         | 360/5307 [00:16<03:28, 23.72it/s]\u001b[A\n",
            "  7%|▋         | 363/5307 [00:16<03:40, 22.40it/s]\u001b[A\n",
            "  7%|▋         | 366/5307 [00:16<03:24, 24.18it/s]\u001b[A\n",
            "  7%|▋         | 370/5307 [00:16<03:03, 26.86it/s]\u001b[A\n",
            "  7%|▋         | 373/5307 [00:16<03:20, 24.55it/s]\u001b[A\n",
            "  7%|▋         | 376/5307 [00:17<03:35, 22.93it/s]\u001b[A\n",
            "  7%|▋         | 379/5307 [00:17<03:44, 21.93it/s]\u001b[A\n",
            "  7%|▋         | 382/5307 [00:17<03:48, 21.59it/s]\u001b[A\n",
            "  7%|▋         | 385/5307 [00:17<03:54, 21.02it/s]\u001b[A\n",
            "  7%|▋         | 388/5307 [00:17<03:55, 20.89it/s]\u001b[A\n",
            "  7%|▋         | 391/5307 [00:17<04:02, 20.28it/s]\u001b[A\n",
            "  7%|▋         | 394/5307 [00:17<03:55, 20.87it/s]\u001b[A\n",
            "  7%|▋         | 397/5307 [00:18<03:37, 22.56it/s]\u001b[A\n",
            "  8%|▊         | 400/5307 [00:18<03:36, 22.64it/s]\u001b[A\n",
            "  8%|▊         | 403/5307 [00:18<03:21, 24.39it/s]\u001b[A\n",
            "  8%|▊         | 406/5307 [00:18<03:31, 23.16it/s]\u001b[A\n",
            "  8%|▊         | 409/5307 [00:18<03:38, 22.39it/s]\u001b[A\n",
            "  8%|▊         | 412/5307 [00:18<03:43, 21.92it/s]\u001b[A\n",
            "  8%|▊         | 415/5307 [00:18<03:55, 20.76it/s]\u001b[A\n",
            "  8%|▊         | 418/5307 [00:19<03:56, 20.64it/s]\u001b[A\n",
            "  8%|▊         | 421/5307 [00:19<03:59, 20.41it/s]\u001b[A\n",
            "  8%|▊         | 424/5307 [00:19<03:46, 21.52it/s]\u001b[A\n",
            "  8%|▊         | 428/5307 [00:19<03:24, 23.82it/s]\u001b[A\n",
            "  8%|▊         | 431/5307 [00:19<03:24, 23.82it/s]\u001b[A\n",
            "  8%|▊         | 434/5307 [00:19<03:34, 22.69it/s]\u001b[A\n",
            "  8%|▊         | 437/5307 [00:19<03:47, 21.43it/s]\u001b[A\n",
            "  8%|▊         | 440/5307 [00:20<03:52, 20.96it/s]\u001b[A\n",
            "  8%|▊         | 443/5307 [00:20<03:34, 22.70it/s]\u001b[A\n",
            "  8%|▊         | 446/5307 [00:20<03:31, 22.97it/s]\u001b[A\n",
            "  8%|▊         | 449/5307 [00:20<03:42, 21.88it/s]\u001b[A\n",
            "  9%|▊         | 452/5307 [00:20<03:49, 21.12it/s]\u001b[A\n",
            "  9%|▊         | 455/5307 [00:20<03:54, 20.67it/s]\u001b[A\n",
            "  9%|▊         | 458/5307 [00:20<03:47, 21.32it/s]\u001b[A\n",
            "  9%|▊         | 461/5307 [00:20<03:29, 23.10it/s]\u001b[A\n",
            "  9%|▊         | 464/5307 [00:21<03:40, 21.94it/s]\u001b[A\n",
            "  9%|▉         | 467/5307 [00:21<03:46, 21.35it/s]\u001b[A\n",
            "  9%|▉         | 470/5307 [00:21<03:33, 22.61it/s]\u001b[A\n",
            "  9%|▉         | 473/5307 [00:21<03:27, 23.34it/s]\u001b[A\n",
            "  9%|▉         | 476/5307 [00:21<03:32, 22.70it/s]\u001b[A\n",
            "  9%|▉         | 480/5307 [00:21<03:13, 24.89it/s]\u001b[A\n",
            "  9%|▉         | 483/5307 [00:21<03:27, 23.26it/s]\u001b[A\n",
            "  9%|▉         | 486/5307 [00:22<03:33, 22.60it/s]\u001b[A\n",
            "  9%|▉         | 489/5307 [00:22<03:39, 21.98it/s]\u001b[A\n",
            "  9%|▉         | 492/5307 [00:22<03:41, 21.71it/s]\u001b[A\n",
            "  9%|▉         | 495/5307 [00:22<03:47, 21.19it/s]\u001b[A\n",
            "  9%|▉         | 498/5307 [00:22<03:36, 22.19it/s]\u001b[A\n",
            "  9%|▉         | 501/5307 [00:22<03:37, 22.09it/s]\u001b[A\n",
            " 10%|▉         | 505/5307 [00:22<03:14, 24.68it/s]\u001b[A\n",
            " 10%|▉         | 508/5307 [00:22<03:25, 23.31it/s]\u001b[A\n",
            " 10%|▉         | 511/5307 [00:23<03:20, 23.91it/s]\u001b[A\n",
            " 10%|▉         | 514/5307 [00:23<03:30, 22.78it/s]\u001b[A\n",
            " 10%|▉         | 517/5307 [00:23<03:39, 21.86it/s]\u001b[A\n",
            " 10%|▉         | 520/5307 [00:23<03:40, 21.71it/s]\u001b[A\n",
            " 10%|▉         | 524/5307 [00:23<03:25, 23.33it/s]\u001b[A\n",
            " 10%|▉         | 527/5307 [00:23<03:29, 22.78it/s]\u001b[A\n",
            " 10%|▉         | 530/5307 [00:23<03:42, 21.49it/s]\u001b[A\n",
            " 10%|█         | 533/5307 [00:24<03:47, 20.97it/s]\u001b[A\n",
            " 10%|█         | 536/5307 [00:24<03:39, 21.74it/s]\u001b[A\n",
            " 10%|█         | 539/5307 [00:24<03:40, 21.63it/s]\u001b[A\n",
            " 10%|█         | 542/5307 [00:24<03:47, 20.95it/s]\u001b[A\n",
            " 10%|█         | 545/5307 [00:24<03:51, 20.60it/s]\u001b[A\n",
            " 10%|█         | 548/5307 [00:24<03:50, 20.66it/s]\u001b[A\n",
            " 10%|█         | 551/5307 [00:25<03:56, 20.08it/s]\u001b[A\n",
            " 10%|█         | 554/5307 [00:25<03:57, 20.04it/s]\u001b[A\n",
            " 10%|█         | 557/5307 [00:25<03:44, 21.12it/s]\u001b[A\n",
            " 11%|█         | 560/5307 [00:25<03:34, 22.14it/s]\u001b[A\n",
            " 11%|█         | 563/5307 [00:25<03:18, 23.87it/s]\u001b[A\n",
            " 11%|█         | 566/5307 [00:25<03:15, 24.21it/s]\u001b[A\n",
            " 11%|█         | 569/5307 [00:25<03:22, 23.44it/s]\u001b[A\n",
            " 11%|█         | 573/5307 [00:25<03:11, 24.77it/s]\u001b[A\n",
            " 11%|█         | 576/5307 [00:26<03:04, 25.68it/s]\u001b[A\n",
            " 11%|█         | 579/5307 [00:26<03:07, 25.18it/s]\u001b[A\n",
            " 11%|█         | 582/5307 [00:26<03:18, 23.78it/s]\u001b[A\n",
            " 11%|█         | 586/5307 [00:26<03:10, 24.80it/s]\u001b[A\n",
            " 11%|█         | 589/5307 [00:26<03:23, 23.18it/s]\u001b[A\n",
            " 11%|█         | 592/5307 [00:26<03:29, 22.52it/s]\u001b[A\n",
            " 11%|█         | 595/5307 [00:26<03:31, 22.33it/s]\u001b[A\n",
            " 11%|█▏        | 599/5307 [00:26<03:10, 24.69it/s]\u001b[A\n",
            " 11%|█▏        | 603/5307 [00:27<03:00, 26.03it/s]\u001b[A\n",
            " 11%|█▏        | 606/5307 [00:27<03:14, 24.17it/s]\u001b[A\n",
            " 11%|█▏        | 610/5307 [00:27<02:51, 27.42it/s]\u001b[A\n",
            " 12%|█▏        | 614/5307 [00:27<02:49, 27.67it/s]\u001b[A\n",
            " 12%|█▏        | 617/5307 [00:27<03:09, 24.79it/s]\u001b[A\n",
            " 12%|█▏        | 620/5307 [00:27<03:21, 23.31it/s]\u001b[A\n",
            " 12%|█▏        | 623/5307 [00:27<03:28, 22.51it/s]\u001b[A\n",
            " 12%|█▏        | 626/5307 [00:28<03:40, 21.22it/s]\u001b[A\n",
            " 12%|█▏        | 629/5307 [00:28<03:41, 21.09it/s]\u001b[A\n",
            " 12%|█▏        | 632/5307 [00:28<03:31, 22.12it/s]\u001b[A\n",
            " 12%|█▏        | 635/5307 [00:28<03:32, 21.94it/s]\u001b[A\n",
            " 12%|█▏        | 638/5307 [00:28<03:37, 21.44it/s]\u001b[A\n",
            " 12%|█▏        | 641/5307 [00:28<03:34, 21.72it/s]\u001b[A\n",
            " 12%|█▏        | 644/5307 [00:28<03:32, 21.99it/s]\u001b[A\n",
            " 12%|█▏        | 648/5307 [00:29<03:15, 23.82it/s]\u001b[A\n",
            " 12%|█▏        | 651/5307 [00:29<03:07, 24.78it/s]\u001b[A\n",
            " 12%|█▏        | 654/5307 [00:29<02:59, 25.94it/s]\u001b[A\n",
            " 12%|█▏        | 657/5307 [00:29<02:59, 25.86it/s]\u001b[A\n",
            " 12%|█▏        | 660/5307 [00:29<03:02, 25.50it/s]\u001b[A\n",
            " 12%|█▏        | 663/5307 [00:29<03:06, 24.86it/s]\u001b[A\n",
            " 13%|█▎        | 666/5307 [00:29<02:57, 26.17it/s]\u001b[A\n",
            " 13%|█▎        | 669/5307 [00:29<02:59, 25.91it/s]\u001b[A\n",
            " 13%|█▎        | 672/5307 [00:29<03:11, 24.19it/s]\u001b[A\n",
            " 13%|█▎        | 675/5307 [00:30<03:25, 22.56it/s]\u001b[A\n",
            " 13%|█▎        | 678/5307 [00:30<03:25, 22.49it/s]\u001b[A\n",
            " 13%|█▎        | 681/5307 [00:30<03:35, 21.44it/s]\u001b[A\n",
            " 13%|█▎        | 685/5307 [00:30<03:05, 24.89it/s]\u001b[A\n",
            " 13%|█▎        | 688/5307 [00:30<03:06, 24.73it/s]\u001b[A\n",
            " 13%|█▎        | 691/5307 [00:30<03:08, 24.51it/s]\u001b[A\n",
            " 13%|█▎        | 694/5307 [00:30<03:05, 24.84it/s]\u001b[A\n",
            " 13%|█▎        | 697/5307 [00:30<02:56, 26.09it/s]\u001b[A\n",
            " 13%|█▎        | 700/5307 [00:31<03:14, 23.68it/s]\u001b[A\n",
            " 13%|█▎        | 703/5307 [00:31<03:25, 22.37it/s]\u001b[A\n",
            " 13%|█▎        | 706/5307 [00:31<03:28, 22.10it/s]\u001b[A\n",
            " 13%|█▎        | 709/5307 [00:31<03:23, 22.57it/s]\u001b[A\n",
            " 13%|█▎        | 712/5307 [00:31<03:30, 21.81it/s]\u001b[A\n",
            " 13%|█▎        | 715/5307 [00:31<03:35, 21.32it/s]\u001b[A\n",
            " 14%|█▎        | 718/5307 [00:31<03:17, 23.23it/s]\u001b[A\n",
            " 14%|█▎        | 721/5307 [00:32<03:16, 23.39it/s]\u001b[A\n",
            " 14%|█▎        | 724/5307 [00:32<03:28, 22.03it/s]\u001b[A\n",
            " 14%|█▎        | 727/5307 [00:32<03:27, 22.09it/s]\u001b[A\n",
            " 14%|█▍        | 730/5307 [00:32<03:19, 22.99it/s]\u001b[A\n",
            " 14%|█▍        | 733/5307 [00:32<03:24, 22.37it/s]\u001b[A\n",
            " 14%|█▍        | 737/5307 [00:32<03:02, 25.03it/s]\u001b[A\n",
            " 14%|█▍        | 741/5307 [00:32<02:47, 27.33it/s]\u001b[A\n",
            " 14%|█▍        | 744/5307 [00:33<02:59, 25.49it/s]\u001b[A\n",
            " 14%|█▍        | 747/5307 [00:33<02:57, 25.66it/s]\u001b[A\n",
            " 14%|█▍        | 750/5307 [00:33<03:13, 23.54it/s]\u001b[A\n",
            " 14%|█▍        | 753/5307 [00:33<03:18, 22.93it/s]\u001b[A\n",
            " 14%|█▍        | 756/5307 [00:33<03:23, 22.34it/s]\u001b[A\n",
            " 14%|█▍        | 759/5307 [00:33<03:25, 22.16it/s]\u001b[A\n",
            " 14%|█▍        | 762/5307 [00:33<03:28, 21.83it/s]\u001b[A\n",
            " 14%|█▍        | 766/5307 [00:33<03:05, 24.53it/s]\u001b[A\n",
            " 14%|█▍        | 769/5307 [00:34<03:10, 23.81it/s]\u001b[A\n",
            " 15%|█▍        | 774/5307 [00:34<02:51, 26.40it/s]\u001b[A\n",
            " 15%|█▍        | 777/5307 [00:34<03:03, 24.63it/s]\u001b[A\n",
            " 15%|█▍        | 780/5307 [00:34<03:03, 24.63it/s]\u001b[A\n",
            " 15%|█▍        | 783/5307 [00:34<03:02, 24.84it/s]\u001b[A\n",
            " 15%|█▍        | 786/5307 [00:34<02:57, 25.49it/s]\u001b[A\n",
            " 15%|█▍        | 790/5307 [00:34<02:43, 27.68it/s]\u001b[A\n",
            " 15%|█▍        | 793/5307 [00:34<02:46, 27.10it/s]\u001b[A\n",
            " 15%|█▍        | 796/5307 [00:35<02:59, 25.14it/s]\u001b[A\n",
            " 15%|█▌        | 799/5307 [00:35<03:11, 23.57it/s]\u001b[A\n",
            " 15%|█▌        | 802/5307 [00:35<03:07, 23.98it/s]\u001b[A\n",
            " 15%|█▌        | 805/5307 [00:35<03:10, 23.58it/s]\u001b[A\n",
            " 15%|█▌        | 808/5307 [00:35<03:12, 23.35it/s]\u001b[A\n",
            " 15%|█▌        | 811/5307 [00:35<03:13, 23.28it/s]\u001b[A\n",
            " 15%|█▌        | 816/5307 [00:35<02:47, 26.81it/s]\u001b[A\n",
            " 15%|█▌        | 820/5307 [00:35<02:36, 28.59it/s]\u001b[A\n",
            " 16%|█▌        | 824/5307 [00:36<02:48, 26.62it/s]\u001b[A\n",
            " 16%|█▌        | 827/5307 [00:36<03:01, 24.69it/s]\u001b[A\n",
            " 16%|█▌        | 830/5307 [00:36<03:08, 23.74it/s]\u001b[A\n",
            " 16%|█▌        | 833/5307 [00:36<03:10, 23.51it/s]\u001b[A\n",
            " 16%|█▌        | 836/5307 [00:36<03:16, 22.76it/s]\u001b[A\n",
            " 16%|█▌        | 839/5307 [00:36<03:19, 22.42it/s]\u001b[A\n",
            " 16%|█▌        | 842/5307 [00:37<03:23, 21.92it/s]\u001b[A\n",
            " 16%|█▌        | 845/5307 [00:37<03:22, 22.04it/s]\u001b[A\n",
            " 16%|█▌        | 848/5307 [00:37<03:10, 23.39it/s]\u001b[A\n",
            " 16%|█▌        | 852/5307 [00:37<02:54, 25.51it/s]\u001b[A\n",
            " 16%|█▌        | 855/5307 [00:37<02:53, 25.67it/s]\u001b[A\n",
            " 16%|█▌        | 858/5307 [00:37<02:52, 25.83it/s]\u001b[A\n",
            " 16%|█▌        | 861/5307 [00:37<03:00, 24.69it/s]\u001b[A\n",
            " 16%|█▋        | 864/5307 [00:37<02:55, 25.31it/s]\u001b[A\n",
            " 16%|█▋        | 867/5307 [00:37<02:54, 25.40it/s]\u001b[A\n",
            " 16%|█▋        | 870/5307 [00:38<03:04, 24.00it/s]\u001b[A\n",
            " 16%|█▋        | 873/5307 [00:38<03:03, 24.10it/s]\u001b[A\n",
            " 17%|█▋        | 876/5307 [00:38<03:15, 22.71it/s]\u001b[A\n",
            " 17%|█▋        | 879/5307 [00:38<03:24, 21.63it/s]\u001b[A\n",
            " 17%|█▋        | 882/5307 [00:38<03:28, 21.27it/s]\u001b[A\n",
            " 17%|█▋        | 885/5307 [00:38<03:28, 21.23it/s]\u001b[A\n",
            " 17%|█▋        | 888/5307 [00:38<03:28, 21.17it/s]\u001b[A\n",
            " 17%|█▋        | 891/5307 [00:39<03:22, 21.79it/s]\u001b[A\n",
            " 17%|█▋        | 894/5307 [00:39<03:07, 23.55it/s]\u001b[A\n",
            " 17%|█▋        | 897/5307 [00:39<03:12, 22.86it/s]\u001b[A\n",
            " 17%|█▋        | 900/5307 [00:39<03:11, 22.99it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "I7i_zlgSuKyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}