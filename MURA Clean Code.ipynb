{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MURA Clean Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Corv113DVLKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ]
    },
    {
      "metadata": {
        "id": "SmaRONjkdPJc",
        "colab_type": "code",
        "outputId": "e1cffa4c-b5c2-442f-ae0b-d963ea89872b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cfvs-0PJVXGi",
        "colab_type": "code",
        "outputId": "4b947526-2879-475f-9480-73a5178efc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "#!wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "#!unzip MURA-v1.1.zip\n",
        "#!rm MURA-v1.1.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hezUpQAcVLK6",
        "colab_type": "code",
        "outputId": "5d4a2b19-7bc6-4c37-970b-67a55268f647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "pd.options.display.max_colwidth = 100\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ut2PF0_mOa_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Reading**"
      ]
    },
    {
      "metadata": {
        "id": "dBWtYEWrOeOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paths_n_labels(csv,str_limp):\n",
        "    #make dataframe\n",
        "    studies=pd.read_csv(csv, sep=',',header=None)\n",
        "    #separate study paths and labels of given limp from those of other limps\n",
        "    limp_studies=studies[studies[0].str.contains(str_limp)==True]\n",
        "    #make it a numpy\n",
        "    limp_studies=np.array(limp_studies)\n",
        "    #limp study folder paths\n",
        "    limp_paths=[]\n",
        "    #labels of given limp\n",
        "    limp_labels=[]\n",
        "    for i in tqdm( range(limp_studies.shape[0]) ):\n",
        "        study_path=limp_studies[i][0]\n",
        "        study_label=limp_studies[i][1]\n",
        "        study_files = [f for f in listdir(study_path) if isfile(join(study_path, f))]\n",
        "        for image in study_files:\n",
        "            limp_paths.append(study_path + image)\n",
        "            limp_labels.append(study_label)\n",
        "\n",
        "    limp_paths=np.array(limp_paths)\n",
        "    limp_labels=np.array(limp_labels)\n",
        "\n",
        "    return limp_paths,limp_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d-KrxmIReoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#general function with options for wrist data case, only set wrist_train=True in the case of wrist training data only\n",
        "#for all other limps and for validation data even that of wrist just pass the paths\n",
        "#targ_size is image resizing with default(224,224)\n",
        "#preprocess flag is for using keras preprocessing for images or just resizing\n",
        "def read_images(paths ,targ_size= (224, 224), wrist_train=False, preprocess=False):\n",
        "    images=[]\n",
        "    #load any limp images\n",
        "    if(not wrist_train):\n",
        "        for path in tqdm(paths):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size )\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    #special case for wrist train corrupted data       \n",
        "    else:\n",
        "        #did this because it gave an error at sample  5307 or near it if took all\n",
        "        sample_e=5307\n",
        "        sample_s2=5339\n",
        "        images=[]\n",
        "        for path in tqdm(paths[:sample_e]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "        #new start\n",
        "        for path in tqdm(paths[sample_s2:]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "    #making it a numpy array instead of python list\n",
        "    return (np.array(images))\n",
        "  \n",
        "\n",
        "def wrist_labels(labels):\n",
        "  sample_e=5307\n",
        "  sample_s2=5339\n",
        "  return np.hstack( [ labels[:sample_e], labels[sample_s2:] ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9xFCfDa2aR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_studies='MURA-v1.1/train_labeled_studies.csv'\n",
        "valid_studies='MURA-v1.1/valid_labeled_studies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4QggWXq2iu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #data bias : train\n",
        "# print(\"0 normal, 1 abnormal\")\n",
        "# unique, counts = np.unique(train_labels, return_counts=True)\n",
        "# print(dict(zip(unique, counts)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJ_NG3xJOjfU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(\"0 normal, 1 abnormal\")\n",
        "# unique, counts = np.unique(valid_labels, return_counts=True)\n",
        "# print(dict(zip(unique, counts)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1u2CNQ7VLMC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "R1VohqXoheEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def images_n_labels(limp,preprocess=True):\n",
        "  \n",
        "  print(\"\\nreading studies of \"+ limp + \"\\n\")\n",
        "  print(train_studies)\n",
        "  train_paths,train_labels=paths_n_labels(train_studies,limp)\n",
        "  valid_paths,valid_labels=paths_n_labels(valid_studies,limp)\n",
        "  \n",
        "  print(train_labels.shape)\n",
        "  print(valid_labels.shape)\n",
        "  print(\"reading \"+ limp + \" training images\")\n",
        "  if (limp == \"WRIST\"):\n",
        "    train_labels=wrist_labels(train_labels)\n",
        "    train_imgs= read_images(train_paths,preprocess=preprocess, wrist_train=True)\n",
        "    \n",
        "  else:\n",
        "    train_imgs= read_images(train_paths,preprocess=preprocess, wrist_train=False)\n",
        "  print(train_imgs.shape)  \n",
        "  print(\"reading \"+ limp + \" validation images\")\n",
        "  valid_imgs= read_images(valid_paths,preprocess=True)\n",
        "  print(valid_imgs.shape)\n",
        "  \n",
        "  return train_imgs, train_labels, valid_imgs, valid_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMST4D272Whd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=True):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #default because refrenced before assignment error, just scroll down\n",
        "  base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  \n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetMobile(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "    \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "  if(add_denses):\n",
        "    # let's add a fully-connected layer\n",
        "    #x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    #x = Dense(32, activation='relu')(x)\n",
        "    # and a logistic layer -- let's say we have 200 classes\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "  else:\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sjcx0lP3srt7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Generator**"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WD21pIOMvQyY"
      },
      "cell_type": "markdown",
      "source": [
        "TODO #K : the runtime died issue when trying to fit the generated data first before using it in fit_generator"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PFkebUq3vP-f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(  rescale=1./255,\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uR-g2EsQcnmm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generalizing over all limps"
      ]
    },
    {
      "metadata": {
        "id": "eXYxHrw_cqIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pass to model list of limps because if wanted to train on less\n",
        "#function outputs a dictionary or dataframe has train and val accuracies for each limp using a chosen model\n",
        "limps=[\"SHOULDER\", \"WRIST\",\"FINGER\", \"ELBOW\", \"HUMERUS\",\"HAND\", \"FOREARM\"]\n",
        "\n",
        "\n",
        "def evaluate_limps(models=[1],epoch=5,batch=32, imagenet=True, freeze_all=False,v=1 , limps=[\"WRIST\"], preprocess_ip=True, augment=False):\n",
        "  accuracies={}\n",
        "  for model in models:\n",
        "    print(\"\\n\\n used base model: \\n\\n\"+bases[model])\n",
        "    for limp in limps:\n",
        "      #print(\"reading \"+ limp + \" images\\n\")\n",
        "      train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp, preprocess=preprocess_ip)\n",
        "      print(\"making model\")\n",
        "      model=make_FT_model(base= model, imagenet=False, freeze_all=False, add_denses=True)\n",
        "      print(\"compiling\")\n",
        "      model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "      ##############################################################\n",
        "\n",
        "      if(augment): #dataaugmentation model fitting\n",
        "        print(\"Augmenting Input data\")\n",
        "        # # compute quantities required for featurewise normalization\n",
        "        # # (std, mean, and principal components if ZCA whitening is applied)\n",
        "\n",
        "        #       datagen.fit(train_imgs)\n",
        "        model.fit_generator(datagen.flow(train_imgs, train_labels, batch_size=16),\n",
        "                      steps_per_epoch=len(train_imgs) / 16, epochs=epoch,use_multiprocessing=False,workers=6,validation_data=None)\n",
        "        print(\"training augmentation calculations for \"+ limp)\n",
        "\n",
        "        loss_tr, accuracy_tr =model.evaluate_generator(datagen.flow(train_imgs,train_labels), use_multiprocessing=True,steps=len(datagen) / 16)\n",
        "\n",
        "        print(\"calculating validation augmentation loss for \"+ limp)\n",
        "\n",
        "        loss_val, accuracy_val = model.evaluate_generator(datagen.flow(valid_imgs,valid_labels), use_multiprocessing=True,steps=len(datagen) / 16)\n",
        "\n",
        "      ###############################################################\n",
        "      else:\n",
        "        print(\"fitting\")\n",
        "        model.fit(train_imgs, train_labels, epochs=epoch, validation_data=(valid_imgs, valid_labels), shuffle=True, verbose=v, batch_size=batch )\n",
        "        print(\"training loss calculations for \"+ limp)\n",
        "        \n",
        "        loss_tr, accuracy_tr =model.evaluate(x=train_imgs, y=train_labels, batch_size=128, verbose=v)\n",
        "        \n",
        "        print(\"calculating validation loss for \"+ limp)\n",
        "        \n",
        "        loss_val, accuracy_val =model.evaluate(x=valid_imgs, y=valid_labels, batch_size=128, verbose=v)\n",
        "        \n",
        "      accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "      print(\"\\n \\n \"+ limp)\n",
        "      print(accuracies)\n",
        "      pd.DataFrame(accuracies).head(2)\n",
        "  return accuracies\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGKgTYYMUcxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Attributes of Experiment { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "imagenet = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#parameter setting for experiment\n",
        "#choose here which limps to use when running the function\n",
        "limps=[ \"ELBOW\", \"FINGER\", \"FOREARM\", \"HUMERUS\",\"HAND\", \"SHOULDER\", \"WRIST\"]\n",
        "\n",
        "\n",
        "# which Transfer Learning base models to use\n",
        "base=[2]\n",
        "#this is just to know index of each model when choosing base above and to print it when running the function\n",
        "bases=[\"MobileNetV2\", \"DenseNet169\", \"InceptionV3\", \"ResNet50\",\"NASNetMobile\"]\n",
        "\n",
        "\n",
        "# whether to use imagenet weights of not\n",
        "imagenet=True\n",
        "\n",
        "\n",
        "# whether to freeze ALL layers of base model or not\n",
        "freeze_all=False\n",
        "\n",
        "\n",
        "# how much information to display about epochs and progress , 0= none , 1 is line per epoch\n",
        "verbose=1\n",
        "\n",
        "\n",
        "# whether to preprocess input \n",
        "preprocess_ip=True\n",
        "\n",
        "\n",
        "#whether to augment data\n",
        "augmentation=False\n",
        "\n",
        "epoch_=15\n",
        "batch=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxKTrlBCt5S9",
        "colab_type": "code",
        "outputId": "c981e697-73e1-410f-aed1-f6721c9fd8be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5160
        }
      },
      "cell_type": "code",
      "source": [
        "R=evaluate_limps(models=base,epoch=epoch_,batch=batch, imagenet=imagenet, freeze_all=freeze_all, v=verbose, limps=limps, preprocess_ip=preprocess_ip, augment=augmentation)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 28/1754 [00:00<00:06, 276.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " used base model: \n",
            "\n",
            "InceptionV3\n",
            "\n",
            "reading studies of ELBOW\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1754/1754 [00:01<00:00, 1460.96it/s]\n",
            "100%|██████████| 158/158 [00:00<00:00, 1224.80it/s]\n",
            "  0%|          | 8/4931 [00:00<01:02, 78.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4931,)\n",
            "(465,)\n",
            "reading ELBOW training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4931/4931 [00:31<00:00, 154.98it/s]\n",
            "  3%|▎         | 15/465 [00:00<00:03, 149.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4931, 224, 224, 3)\n",
            "reading ELBOW validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 465/465 [00:13<00:00, 26.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(465, 224, 224, 3)\n",
            "making model\n",
            "compiling\n",
            "fitting\n",
            "Train on 4931 samples, validate on 465 samples\n",
            "Epoch 1/15\n",
            "4931/4931 [==============================] - 127s 26ms/step - loss: 0.7637 - acc: 0.5865 - val_loss: 0.8375 - val_acc: 0.5054\n",
            "Epoch 2/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.6688 - acc: 0.6131 - val_loss: 2.7314 - val_acc: 0.4559\n",
            "Epoch 3/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.6431 - acc: 0.6508 - val_loss: 0.8351 - val_acc: 0.5871\n",
            "Epoch 4/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.6536 - acc: 0.6072 - val_loss: 0.7674 - val_acc: 0.5247\n",
            "Epoch 5/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.6263 - acc: 0.6534 - val_loss: 1.3880 - val_acc: 0.5935\n",
            "Epoch 6/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.6078 - acc: 0.6808 - val_loss: 0.6765 - val_acc: 0.6237\n",
            "Epoch 7/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5834 - acc: 0.7104 - val_loss: 0.8928 - val_acc: 0.6065\n",
            "Epoch 8/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5528 - acc: 0.7327 - val_loss: 0.5890 - val_acc: 0.6946\n",
            "Epoch 9/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5269 - acc: 0.7483 - val_loss: 0.6535 - val_acc: 0.6323\n",
            "Epoch 10/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5328 - acc: 0.7495 - val_loss: 0.9454 - val_acc: 0.5699\n",
            "Epoch 11/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5223 - acc: 0.7544 - val_loss: 0.5491 - val_acc: 0.6989\n",
            "Epoch 12/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5074 - acc: 0.7712 - val_loss: 0.6051 - val_acc: 0.6753\n",
            "Epoch 13/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5048 - acc: 0.7714 - val_loss: 0.7448 - val_acc: 0.5720\n",
            "Epoch 14/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5511 - acc: 0.7311 - val_loss: 0.6979 - val_acc: 0.5333\n",
            "Epoch 15/15\n",
            "4931/4931 [==============================] - 96s 20ms/step - loss: 0.5116 - acc: 0.7656 - val_loss: 0.7249 - val_acc: 0.5806\n",
            "training loss calculations for ELBOW\n",
            "4931/4931 [==============================] - 31s 6ms/step\n",
            "calculating validation loss for ELBOW\n",
            "465/465 [==============================] - 5s 10ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 42/1935 [00:00<00:04, 391.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " ELBOW\n",
            "{'ELBOW': [0.655039545731089, 0.5806451612903226]}\n",
            "\n",
            "reading studies of FINGER\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1935/1935 [00:01<00:00, 1699.06it/s]\n",
            "100%|██████████| 175/175 [00:00<00:00, 4130.30it/s]\n",
            "  0%|          | 15/5106 [00:00<00:34, 149.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(5106,)\n",
            "(461,)\n",
            "reading FINGER training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5106/5106 [00:34<00:00, 150.01it/s]\n",
            "  3%|▎         | 16/461 [00:00<00:02, 156.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(5106, 224, 224, 3)\n",
            "reading FINGER validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 461/461 [00:03<00:00, 129.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(461, 224, 224, 3)\n",
            "making model\n",
            "compiling\n",
            "fitting\n",
            "Train on 5106 samples, validate on 461 samples\n",
            "Epoch 1/15\n",
            "5106/5106 [==============================] - 80s 16ms/step - loss: 0.6099 - acc: 0.6620 - val_loss: 3.2733 - val_acc: 0.6030\n",
            "Epoch 2/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.5678 - acc: 0.6859 - val_loss: 1.9165 - val_acc: 0.6377\n",
            "Epoch 3/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.5373 - acc: 0.7099 - val_loss: 2.0939 - val_acc: 0.6876\n",
            "Epoch 4/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.5259 - acc: 0.7229 - val_loss: 1.2981 - val_acc: 0.6681\n",
            "Epoch 5/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.5078 - acc: 0.7389 - val_loss: 2.1576 - val_acc: 0.6399\n",
            "Epoch 6/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4992 - acc: 0.7444 - val_loss: 2.8349 - val_acc: 0.5662\n",
            "Epoch 7/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4899 - acc: 0.7534 - val_loss: 3.2921 - val_acc: 0.5228\n",
            "Epoch 8/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4716 - acc: 0.7648 - val_loss: 1.8614 - val_acc: 0.6551\n",
            "Epoch 9/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4527 - acc: 0.7799 - val_loss: 2.1141 - val_acc: 0.6226\n",
            "Epoch 10/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4492 - acc: 0.7902 - val_loss: 6.8124 - val_acc: 0.5358\n",
            "Epoch 11/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4279 - acc: 0.8053 - val_loss: 7.3967 - val_acc: 0.5358\n",
            "Epoch 12/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.4053 - acc: 0.8114 - val_loss: 3.4704 - val_acc: 0.5987\n",
            "Epoch 13/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.3870 - acc: 0.8224 - val_loss: 2.7995 - val_acc: 0.6139\n",
            "Epoch 14/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.3890 - acc: 0.8249 - val_loss: 7.2515 - val_acc: 0.5315\n",
            "Epoch 15/15\n",
            "5106/5106 [==============================] - 63s 12ms/step - loss: 0.3530 - acc: 0.8429 - val_loss: 7.0034 - val_acc: 0.5380\n",
            "training loss calculations for FINGER\n",
            "5106/5106 [==============================] - 17s 3ms/step\n",
            "calculating validation loss for FINGER\n",
            "461/461 [==============================] - 2s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 51/877 [00:00<00:01, 505.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " FINGER\n",
            "{'ELBOW': [0.655039545731089, 0.5806451612903226], 'FINGER': [0.38601645123384254, 0.5379609544448345]}\n",
            "\n",
            "reading studies of FOREARM\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 877/877 [00:00<00:00, 1600.48it/s]\n",
            "100%|██████████| 133/133 [00:00<00:00, 1479.28it/s]\n",
            "  1%|          | 10/1825 [00:00<00:18, 96.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1825,)\n",
            "(301,)\n",
            "reading FOREARM training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1825/1825 [00:18<00:00, 98.41it/s] \n",
            "  4%|▍         | 12/301 [00:00<00:02, 113.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1825, 224, 224, 3)\n",
            "reading FOREARM validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 301/301 [00:02<00:00, 145.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(301, 224, 224, 3)\n",
            "making model\n",
            "compiling\n",
            "fitting\n",
            "Train on 1825 samples, validate on 301 samples\n",
            "Epoch 1/15\n",
            "1825/1825 [==============================] - 41s 22ms/step - loss: 0.6934 - acc: 0.6181 - val_loss: 2.7347 - val_acc: 0.5116\n",
            "Epoch 2/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.6314 - acc: 0.6674 - val_loss: 2.6195 - val_acc: 0.4983\n",
            "Epoch 3/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.6121 - acc: 0.6707 - val_loss: 1.2321 - val_acc: 0.5415\n",
            "Epoch 4/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.5718 - acc: 0.7211 - val_loss: 1.2529 - val_acc: 0.4983\n",
            "Epoch 5/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.5415 - acc: 0.7375 - val_loss: 5.6622 - val_acc: 0.4983\n",
            "Epoch 6/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.4979 - acc: 0.7682 - val_loss: 4.7274 - val_acc: 0.5017\n",
            "Epoch 7/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.5044 - acc: 0.7748 - val_loss: 1.3834 - val_acc: 0.5349\n",
            "Epoch 8/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.5018 - acc: 0.7688 - val_loss: 2.3302 - val_acc: 0.5548\n",
            "Epoch 9/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.4088 - acc: 0.8290 - val_loss: 3.8931 - val_acc: 0.5017\n",
            "Epoch 10/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.3865 - acc: 0.8296 - val_loss: 3.0479 - val_acc: 0.5648\n",
            "Epoch 11/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.3202 - acc: 0.8734 - val_loss: 5.8487 - val_acc: 0.5050\n",
            "Epoch 12/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.3661 - acc: 0.8400 - val_loss: 2.0340 - val_acc: 0.5781\n",
            "Epoch 13/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.3115 - acc: 0.8701 - val_loss: 4.9102 - val_acc: 0.5017\n",
            "Epoch 14/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.2185 - acc: 0.9173 - val_loss: 4.4173 - val_acc: 0.5515\n",
            "Epoch 15/15\n",
            "1825/1825 [==============================] - 23s 13ms/step - loss: 0.2567 - acc: 0.8948 - val_loss: 3.7810 - val_acc: 0.5249\n",
            "training loss calculations for FOREARM\n",
            "1825/1825 [==============================] - 6s 3ms/step\n",
            "calculating validation loss for FOREARM\n",
            "301/301 [==============================] - 1s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/592 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " FOREARM\n",
            "{'ELBOW': [0.655039545731089, 0.5806451612903226], 'FINGER': [0.38601645123384254, 0.5379609544448345], 'FOREARM': [0.5605479449441988, 0.5249169462939037]}\n",
            "\n",
            "reading studies of HUMERUS\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 592/592 [00:00<00:00, 1406.75it/s]\n",
            "100%|██████████| 135/135 [00:00<00:00, 1379.73it/s]\n",
            "  1%|          | 13/1272 [00:00<00:10, 125.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1272,)\n",
            "(288,)\n",
            "reading HUMERUS training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1272/1272 [00:12<00:00, 103.52it/s]\n",
            "  5%|▍         | 13/288 [00:00<00:02, 118.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1272, 224, 224, 3)\n",
            "reading HUMERUS validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 288/288 [00:01<00:00, 151.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(288, 224, 224, 3)\n",
            "making model\n",
            "compiling\n",
            "fitting\n",
            "Train on 1272 samples, validate on 288 samples\n",
            "Epoch 1/15\n",
            "1272/1272 [==============================] - 38s 30ms/step - loss: 0.7433 - acc: 0.5684 - val_loss: 3.1933 - val_acc: 0.4861\n",
            "Epoch 2/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.6533 - acc: 0.6250 - val_loss: 2.2444 - val_acc: 0.4757\n",
            "Epoch 3/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.6234 - acc: 0.6690 - val_loss: 1.8718 - val_acc: 0.4826\n",
            "Epoch 4/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.6039 - acc: 0.6698 - val_loss: 3.1428 - val_acc: 0.4861\n",
            "Epoch 5/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.5389 - acc: 0.7154 - val_loss: 6.4284 - val_acc: 0.4861\n",
            "Epoch 6/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.5207 - acc: 0.7414 - val_loss: 5.3412 - val_acc: 0.4861\n",
            "Epoch 7/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.4778 - acc: 0.7649 - val_loss: 6.8445 - val_acc: 0.4861\n",
            "Epoch 8/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.3744 - acc: 0.8278 - val_loss: 8.1763 - val_acc: 0.4861\n",
            "Epoch 9/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.4027 - acc: 0.8137 - val_loss: 6.5361 - val_acc: 0.4861\n",
            "Epoch 10/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.3710 - acc: 0.8420 - val_loss: 8.1926 - val_acc: 0.4861\n",
            "Epoch 11/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.3028 - acc: 0.8695 - val_loss: 8.1926 - val_acc: 0.4861\n",
            "Epoch 12/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.2761 - acc: 0.8915 - val_loss: 8.1926 - val_acc: 0.4861\n",
            "Epoch 13/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.2312 - acc: 0.9009 - val_loss: 8.1926 - val_acc: 0.4861\n",
            "Epoch 14/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.1617 - acc: 0.9465 - val_loss: 8.1926 - val_acc: 0.4861\n",
            "Epoch 15/15\n",
            "1272/1272 [==============================] - 16s 13ms/step - loss: 0.1509 - acc: 0.9458 - val_loss: 8.1926 - val_acc: 0.4861\n",
            "training loss calculations for HUMERUS\n",
            "1272/1272 [==============================] - 5s 4ms/step\n",
            "calculating validation loss for HUMERUS\n",
            "288/288 [==============================] - 1s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2018 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " HUMERUS\n",
            "{'ELBOW': [0.655039545731089, 0.5806451612903226], 'FINGER': [0.38601645123384254, 0.5379609544448345], 'FOREARM': [0.5605479449441988, 0.5249169462939037], 'HUMERUS': [0.47091194968553457, 0.4861111111111111]}\n",
            "\n",
            "reading studies of HAND\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2018/2018 [00:01<00:00, 1631.31it/s]\n",
            "100%|██████████| 167/167 [00:00<00:00, 1563.19it/s]\n",
            "  0%|          | 13/5543 [00:00<00:46, 118.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(5543,)\n",
            "(460,)\n",
            "reading HAND training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5543/5543 [02:12<00:00, 41.70it/s]\n",
            "  3%|▎         | 16/460 [00:00<00:02, 158.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(5543, 224, 224, 3)\n",
            "reading HAND validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 460/460 [00:03<00:00, 146.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(460, 224, 224, 3)\n",
            "making model\n",
            "compiling\n",
            "fitting\n",
            "Train on 5543 samples, validate on 460 samples\n",
            "Epoch 1/15\n",
            "5543/5543 [==============================] - 93s 17ms/step - loss: 0.5945 - acc: 0.7247 - val_loss: 1.0913 - val_acc: 0.5891\n",
            "Epoch 2/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.5648 - acc: 0.7359 - val_loss: 2.2662 - val_acc: 0.5826\n",
            "Epoch 3/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.5476 - acc: 0.7445 - val_loss: 1.3940 - val_acc: 0.5891\n",
            "Epoch 4/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.5351 - acc: 0.7509 - val_loss: 3.1341 - val_acc: 0.5891\n",
            "Epoch 5/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.5330 - acc: 0.7588 - val_loss: 2.4981 - val_acc: 0.5696\n",
            "Epoch 6/15\n",
            "5543/5543 [==============================] - 70s 13ms/step - loss: 0.5246 - acc: 0.7660 - val_loss: 2.0338 - val_acc: 0.5891\n",
            "Epoch 7/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.5146 - acc: 0.7660 - val_loss: 3.6541 - val_acc: 0.5891\n",
            "Epoch 8/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.5081 - acc: 0.7703 - val_loss: 2.3686 - val_acc: 0.5891\n",
            "Epoch 9/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4972 - acc: 0.7799 - val_loss: 2.1691 - val_acc: 0.5891\n",
            "Epoch 10/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4976 - acc: 0.7819 - val_loss: 2.2572 - val_acc: 0.5891\n",
            "Epoch 11/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4778 - acc: 0.7914 - val_loss: 1.7290 - val_acc: 0.5891\n",
            "Epoch 12/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4705 - acc: 0.7925 - val_loss: 2.1860 - val_acc: 0.5891\n",
            "Epoch 13/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4578 - acc: 0.8012 - val_loss: 2.9641 - val_acc: 0.5891\n",
            "Epoch 14/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4419 - acc: 0.8088 - val_loss: 3.3045 - val_acc: 0.5891\n",
            "Epoch 15/15\n",
            "5543/5543 [==============================] - 69s 12ms/step - loss: 0.4324 - acc: 0.8095 - val_loss: 3.2946 - val_acc: 0.5891\n",
            "training loss calculations for HAND\n",
            "5543/5543 [==============================] - 17s 3ms/step\n",
            "calculating validation loss for HAND\n",
            "460/460 [==============================] - 2s 4ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 17/2821 [00:00<00:16, 169.30it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " HAND\n",
            "{'ELBOW': [0.655039545731089, 0.5806451612903226], 'FINGER': [0.38601645123384254, 0.5379609544448345], 'FOREARM': [0.5605479449441988, 0.5249169462939037], 'HUMERUS': [0.47091194968553457, 0.4861111111111111], 'HAND': [0.7322749413674905, 0.5891304347826087]}\n",
            "\n",
            "reading studies of SHOULDER\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2821/2821 [00:02<00:00, 1162.27it/s]\n",
            "100%|██████████| 194/194 [00:00<00:00, 1024.67it/s]\n",
            "  0%|          | 14/8379 [00:00<01:04, 129.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8379,)\n",
            "(563,)\n",
            "reading SHOULDER training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8379/8379 [01:38<00:00, 85.37it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0EoZu9YVVFJH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Install the PyDrive wrapper & import libraries.\n",
        "# # This only needs to be done once in a notebook.\n",
        "# !pip install -U -q PyDrive\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once in a notebook.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "# # Create & upload a text file.\n",
        "# uploaded = drive.CreateFile({'title': 'Sample file.txt'})\n",
        "# uploaded.SetContentString('Sample upload file content')\n",
        "# uploaded.Upload()\n",
        "# print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMcyBZz_yDvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(R).head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7i_zlgSuKyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#may need to make a function to evaluate all limps over one model together as one dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_9VVa8XVLMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Current point: generalizing functions and code cleaning+ seeing early stopping callback**"
      ]
    },
    {
      "metadata": {
        "id": "vt5njgtKmMOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All unsolved problems:\n",
        "\n",
        "* which layers to freeze and which to train + should I train TL before freezing it ?\n",
        "* data augmentation to generate more data (solved by khaled,still needs little verification)\n",
        "* recording variation in accuracy after every change to get intuition\n",
        "* what does outputted loss represent ? how to read the number ?\n",
        "* normalization step and its effect on accuracy\n",
        "* should I use 1 or two neurons at output layer ?\n",
        "* binary crossentropy weights\n",
        "* justifying parameter use and discovering useful params\n",
        "* try training with model unfrozen with imagenet and without it\n",
        "* make a function to record and tabulate outputs\n",
        "* could we add precision or recall metric ? change accuracy?\n",
        "* why doesn't it work if removed GlobalAveragePooling line?\n",
        "* see if want to freeze less layers\n",
        "* generalize file reading functions\n",
        "* use better batch size ? increase epochs ?\n",
        "* training function with preprocessing flag and multiple model comparisons\n",
        "* grid search like function to tune models and hyperparameters\n",
        "* early stopping callback and save model to drive for continuing training\n",
        "* feature concatenation\n",
        "* ensemble model\n",
        "* maybe we are reading images all wrong and we should read them to grayscale then on preprocessing return them to 3 channels"
      ]
    },
    {
      "metadata": {
        "id": "IVMVHTzRF7f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**results on wrist data: no preprocessing but resize**\n",
        "\n",
        "\n",
        "1- Densenet, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "2- Densenet with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "65% train and 65% validation\n",
        "\n",
        "3-InceptionV3, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "4-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "82% train and 78% validation\n",
        "\n",
        "\n",
        "\n",
        "**results on wrist data: with keras preprocessing **\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them)\n",
        "71% train and 68% validation\n",
        "\n",
        "epoch=6 mins\n",
        "\n",
        "2- Densenet WITHOUT imagenet and didnt freeze \n",
        "\n",
        "62% train and 59% validation\n",
        "\n",
        "epoch=5.5 mins\n",
        "\n",
        "3-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "79% train and 74% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "4-InceptionV3 WITHOUT imagenet and didnt freeze \n",
        "59% train and 57% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "**Shoulder data: with keras preprocessing **\n",
        "\n",
        "1-Densenet with imagenet and no freeze\n",
        "51% train and 51% Validation\n",
        "\n",
        "2-Mobilenet with imagenet and no freeze\n",
        "51% train and 52% Validation\n",
        "\n",
        "3-Mobilenet with imagenet and no freeze 2 DENSES INSTEAD OF 4\n",
        "\n",
        "50% train and 50% Validation\n",
        " also 51% train and 50 Validation\n",
        " \n",
        "**Finger data: with keras preprocessing **\n",
        "\n",
        "1-Densenet with imagenet and no freeze\n",
        "63% train and 69% Validation\n",
        "\n",
        "**Elbow data: with keras preprocessing **\n",
        "\n",
        "1-Densenet with imagenet and no freeze\n",
        "59% train and 50% Validation\n",
        "\n",
        "**Humerus data: with keras preprocessing **\n",
        "\n",
        "\n",
        "1-Densenet with imagenet and no freeze\n",
        "47% train and 49% Validation\n",
        "\n",
        "**Hand data: with keras preprocessing **\n",
        "\n",
        "1-Densenet with imagenet and no freeze\n",
        "73% train and 59% Validation\n",
        "\n",
        "**Forearm data: with keras preprocessing **\n",
        "\n",
        "1-Densenet with imagenet and no freeze\n",
        "64% train and 50% Validation\n",
        "\n",
        "\n",
        "** Conclusions till now **\n",
        "\n",
        "1- keras preprocessing (nomalization and subtracting mean maybe ) with inception reduces  accuracy both training and validation , but with DenseNet it improved accuracy \n",
        "\n",
        "2- Training over imagenet weights without freezing the TL model gives significantly higher accuracy than freezing whole TL and just training denses\n",
        "\n",
        "3-Training TL without imagenet and no freeze till now (2 Trials) seems to get worse results than with imagenet and training over it"
      ]
    },
    {
      "metadata": {
        "id": "2v-AiB5xDDKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Mobilenet Preprocess **\n",
        "\n",
        "{'FINGER': [0.6145710928319624, 0.4642082429501085], \n",
        "\n",
        "'ELBOW': [0.4877306834585501, 0.48817204329916225], \n",
        "\n",
        "'HUMERUS': [0.4716981132075472, 0.4826388888888889], \n",
        "\n",
        "'HAND': [0.7322749413674905, 0.5891304347826087], '\n",
        "\n",
        "FOREARM': [0.36219178082191783, 0.5016611295681063]}\n",
        "\n",
        "** Mobilenet , preprocess , imagenet, no freeze , ADAM **\n",
        "\n",
        "{'FINGER': [0.4001175088160793, 0.5379609544468547],\n",
        "\n",
        "'ELBOW': [0.5096329345081332, 0.43440860163780953],\\\n",
        "\n",
        "'HUMERUS': [0.47091194968553457, 0.4861111111111111], \n",
        "\n",
        "'HAND': [0.7322749413674905, 0.5891304347826087],\n",
        "\n",
        "'FOREARM': [0.6202739730926409, 0.4950166112956811]}\n",
        "\n",
        "** Mobilenet , preprocess , imagenet, no freeze , ADAM , epochs=15**\n",
        "\n",
        "{'ELBOW': [0.5503954573290218, 0.5440860183008256] batch=32,\n",
        "\n",
        "'ELBOW': [0.593185966335429, 0.5053763440860215] batch= 64,\n",
        "\n",
        "'FINGER': [0.513709361395367, 0.622559654286006], \n",
        "\n",
        "'FOREARM': [0.36219178082191783, 0.5016611295681063], \n",
        "\n",
        "'HUMERUS': [0.47091194968553457, 0.4861111111111111]\n",
        "\n",
        ", 'HAND': [0.7322749413674905, 0.5891304347826087]}}\n",
        "\n",
        "** Mobilenet no preprocess **\n",
        "\n",
        "{'FINGER': [0.5084214651299793, 0.5422993490791631],\n",
        "\n",
        "'ELBOW': [0.4068140336645711, 0.4946236559139785], '\n",
        "\n",
        "HUMERUS': [0.47091194968553457, 0.4861111111111111]\n",
        "\n",
        "'HAND': [0.7322749413674905, 0.5891304347826087]\n",
        "\n",
        "'FOREARM': [0.41315068509480724, 0.4983388704318937]\n",
        "\n",
        "'SHOULDER': [0.4970760233544666, 0.5062166962699822]}\n",
        "\n",
        "**Takeaway : preprocessing USUALLY doesnt matter in accuracy**\n",
        "\n",
        "** Mobilenet no preprocess no imagenet no freeze **\n",
        "\n",
        "{'FINGER': [0.428515472011243, 0.46637744034707157],\n",
        "\n",
        "'ELBOW': [0.4068140336645711, 0.4946236559139785],\n",
        "\n",
        "'HUMERUS': [0.47091194968553457, 0.4861111111111111],\n",
        "\n",
        "'HAND': [0.7322749413674905, 0.5891304347826087],\n",
        "\n",
        "'FOREARM': [0.6284931512074928, 0.5016611295681063],\n",
        "\n",
        "'SHOULDER': [0.5177228787674057, 0.5062166962699822],\n",
        "\n",
        "'WRIST': [0.5384615386454278, 0.5523520485584219]}\n",
        "\n",
        "** Mobilenet preprocess, imagenet, froze all **\n",
        "\n",
        "(highest finger,same humerus, same hand, worse same forearm)\n",
        "\n",
        "\n",
        "{'FINGER': [0.6549157851404458, 0.6811279839393632], \n",
        "\n",
        "'ELBOW': [0.41188399917936197, 0.5161290323381783],\n",
        "\n",
        "'HUMERUS': [0.47091194968553457, 0.4861111111111111], \n",
        "\n",
        "'HAND': [0.7322749413674905, 0.5891304347826087],\n",
        "\n",
        "'FOREARM': [0.36219178082191783, 0.5016611295681063]}\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZuYgvFWMDL_H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Inceptionv3 defaults + adam +64 batch\n",
        "\n",
        "{'ELBOW': [0.655039545731089, 0.5806451612903226], 'FINGER': [0.38601645123384254, 0.5379609544448345], 'FOREARM': [0.5605479449441988, 0.5249169462939037], 'HUMERUS': [0.47091194968553457, 0.4861111111111111], 'HAND': [0.7322749413674905, 0.5891304347826087]}"
      ]
    },
    {
      "metadata": {
        "id": "2443ditHkxJw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Mobilenet preprocess, imagenet, froze all \n",
        "print(\"Mobilenet preprocess, imagenet, froze all \")\n",
        "pd.DataFrame({'FINGER': [0.6549157851404458, 0.6811279839393632], 'ELBOW': [0.41188399917936197, 0.5161290323381783],'HUMERUS': [0.47091194968553457, 0.4861111111111111], 'HAND': [0.7322749413674905, 0.5891304347826087],'FOREARM': [0.36219178082191783, 0.5016611295681063]}).head(2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16_OAF1jOsRm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Inception results\n",
        "print(\"inception V3 results, 0 train , 1 val\")\n",
        "pd.DataFrame({'FINGER': [0.6956521734927992, 0.5661605190558444],'ELBOW': [0.5814236464642579, 0.5569892496191046],'HUMERUS': [0.470125786163522, 0.4895833333333333],'HAND': [0.7292080101028324, 0.5934782608695652],'FOREARM': [0.36657534248208345, 0.5016611295681063]}).head(2)\n",
        "\n",
        "#learn how to display all tables in one cell for eyeballing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KY3U-cJkRY-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ResNet50 results\n",
        "print(\"ResNet50 results, 0 train , 1 val\")\n",
        "pd.DataFrame({'ELBOW': [0.593185966335429, 0.5053763440860215]}).head(2)\n",
        "\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XY8Wm8al6gPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Maybe we need to start training over all data not just individual limps that may improve accuracy and remove that underfitting\n",
        "# and maybe we should make reading images explicit and only fit in evaluation function\n",
        "# that may end the disconnection probelm "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}