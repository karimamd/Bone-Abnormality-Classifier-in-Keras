{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MURA Clean Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Corv113DVLKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ]
    },
    {
      "metadata": {
        "id": "SmaRONjkdPJc",
        "colab_type": "code",
        "outputId": "32ce1746-7292-43bb-de10-0df9615c2567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cfvs-0PJVXGi",
        "colab_type": "code",
        "outputId": "28ba30e6-313f-4db6-8198-808f14d60788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# !wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "# !unzip MURA-v1.1.zip\n",
        "# !rm MURA-v1.1.zip\n",
        "!ls"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hezUpQAcVLK6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "pd.options.display.max_colwidth = 100\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ut2PF0_mOa_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Reading**"
      ]
    },
    {
      "metadata": {
        "id": "dBWtYEWrOeOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paths_n_labels(csv,str_limp):\n",
        "    #make dataframe\n",
        "    studies=pd.read_csv(csv, sep=',',header=None)\n",
        "    #separate study paths and labels of given limp from those of other limps\n",
        "    limp_studies=studies[studies[0].str.contains(str_limp)==True]\n",
        "    #make it a numpy\n",
        "    limp_studies=np.array(limp_studies)\n",
        "    #limp study folder paths\n",
        "    limp_paths=[]\n",
        "    #labels of given limp\n",
        "    limp_labels=[]\n",
        "    for i in tqdm( range(limp_studies.shape[0]) ):\n",
        "        study_path=limp_studies[i][0]\n",
        "        study_label=limp_studies[i][1]\n",
        "        study_files = [f for f in listdir(study_path) if isfile(join(study_path, f))]\n",
        "        for image in study_files:\n",
        "            limp_paths.append(study_path + image)\n",
        "            limp_labels.append(study_label)\n",
        "\n",
        "    limp_paths=np.array(limp_paths)\n",
        "    limp_labels=np.array(limp_labels)\n",
        "\n",
        "    return limp_paths,limp_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d-KrxmIReoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#general function with options for wrist data case, only set wrist_train=True in the case of wrist training data only\n",
        "#for all other limps and for validation data even that of wrist just pass the paths\n",
        "#targ_size is image resizing with default(224,224)\n",
        "#preprocess flag is for using keras preprocessing for images or just resizing\n",
        "def read_images(paths ,targ_size= (224, 224), wrist_train=False, preprocess=False):\n",
        "    images=[]\n",
        "    #load any limp images\n",
        "    if(not wrist_train):\n",
        "        for path in tqdm(paths):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size,color_mode = \"grayscale\" )\n",
        "            \n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    #special case for wrist train corrupted data       \n",
        "    else:\n",
        "        #did this because it gave an error at sample  5307 or near it if took all\n",
        "        sample_e=5307\n",
        "        sample_s2=5339\n",
        "        images=[]\n",
        "        for path in tqdm(paths[:sample_e]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size,color_mode = \"grayscale\")\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "        #new start\n",
        "        for path in tqdm(paths[sample_s2:]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size,color_mode = \"grayscale\")\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    \n",
        "    \n",
        "    #making it a numpy array instead of python list\n",
        "    return (np.array(images))\n",
        "  \n",
        "\n",
        "def wrist_labels(labels):\n",
        "  sample_e=5307\n",
        "  sample_s2=5339\n",
        "  return np.hstack( [ labels[:sample_e], labels[sample_s2:] ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9xFCfDa2aR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_studies='MURA-v1.1/train_labeled_studies.csv'\n",
        "valid_studies='MURA-v1.1/valid_labeled_studies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4QggWXq2iu-",
        "colab_type": "code",
        "outputId": "cda694fa-0edc-4194-d42e-4c7487908e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#training images paths and labels\n",
        "#starting with wrist data\n",
        "train_paths,train_labels=paths_n_labels(train_studies,\"WRIST\")\n",
        "#data bias : train\n",
        "print(\"0 normal, 1 abnormal\")\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3460/3460 [00:00<00:00, 19767.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 normal, 1 abnormal\n",
            "{0: 5769, 1: 3987}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "58kzFDVzGzzl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJ_NG3xJOjfU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(\"0 normal, 1 abnormal\")\n",
        "# unique, counts = np.unique(valid_labels, return_counts=True)\n",
        "# print(dict(zip(unique, counts)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1u2CNQ7VLMC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "mMST4D272Whd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=True):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetLarge(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "    \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "  if(add_denses):\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    # and a logistic layer -- let's say we have 200 classes\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "  else:\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1VohqXoheEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def images_n_labels(limp): \n",
        "  \n",
        "  print(\"\\nreading studies of \"+ limp + \"\\n\")\n",
        "  print(train_studies)\n",
        "  train_paths,train_labels=paths_n_labels(train_studies,limp)\n",
        "  valid_paths,valid_labels=paths_n_labels(valid_studies,limp)\n",
        "  \n",
        "  print(train_labels.shape)\n",
        "  print(valid_labels.shape)\n",
        "  print(\"reading \"+ limp + \" training images\")\n",
        "  if (limp == \"WRIST\"):\n",
        "    train_labels=wrist_labels(train_labels) #for wrist reading problem\n",
        "    train_imgs= read_images(train_paths,preprocess=True, wrist_train=True)\n",
        "    \n",
        "  else:\n",
        "    train_imgs= read_images(train_paths,preprocess=True, wrist_train=False)\n",
        "  print(train_imgs.shape)  \n",
        "  print(\"reading \"+ limp + \" validation images\")\n",
        "  valid_imgs= read_images(valid_paths,preprocess=True)\n",
        "  print(valid_imgs.shape)\n",
        "  \n",
        "  print(train_imgs.shape,train_labels.shape) #for shoulder (563, 224, 224, 1) # (8379, 224, 224, 1) (8379,)\n",
        "  return train_imgs, train_labels, valid_imgs, valid_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghAljjlsYGgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The  Generator**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YX4l8VYbVLM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(  rescale=1./255,\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uR-g2EsQcnmm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generalizing over all limps"
      ]
    },
    {
      "metadata": {
        "id": "eXYxHrw_cqIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limps=[\"SHOULDER\", \"WRIST\", \"FINGER\", \"ELBOW\", \"HUMERUS\", \"HAND\", \"FOREARM\"]\n",
        "\n",
        "\n",
        "#pass to model list of limps because if wanted to train on less\n",
        "#function outputs a dictionary or dataframe has train and val accuracies for each limp using a chosen model\n",
        "\n",
        "def evaluate_limps(model_no=2, imagenet=True, freeze_all=False,v=1 , limps=limps, augment=False):\n",
        "  accuracies={}\n",
        "  for limp in limps:\n",
        "    #print(\"reading \"+ limp + \" images\\n\")\n",
        "    train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp)\n",
        "    print(\"making model\")\n",
        "    model=make_FT_model(base= model_no, imagenet=False, freeze_all=False, add_denses=True)\n",
        "    print(\"compiling\")\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    ##############################################################\n",
        "    \n",
        "    if(augment): #dataaugmentation model fitting\n",
        "      print(\"Augmenting Input data\")\n",
        "      # # compute quantities required for featurewise normalization\n",
        "      # # (std, mean, and principal components if ZCA whitening is applied)\n",
        "\n",
        "#       datagen.fit(train_imgs)\n",
        "      model.fit_generator(datagen.flow(train_imgs, train_labels, batch_size=16),\n",
        "                    steps_per_epoch=len(train_imgs) / 16, epochs=5,use_multiprocessing=False,workers=6,validation_data=None)\n",
        "      print(\"training augmentation calculations for \"+ limp)\n",
        "\n",
        "      loss_tr, accuracy_tr =model.evaluate_generator(datagen.flow(train_imgs,train_labels), use_multiprocessing=True,steps=len(valid_imgs) / 16)\n",
        "                                                     \n",
        "      print(\"calculating validation augmentation loss for \"+ limp)\n",
        "                                                     \n",
        "      loss_val, accuracy_val = model.evaluate_generator(datagen.flow(valid_imgs,valid_labels), use_multiprocessing=True,steps=len(valid_imgs) / 16)\n",
        "      \n",
        "    ###############################################################\n",
        "    else:\n",
        "      \n",
        "      print(\"fitting\")\n",
        "      model.fit(train_imgs, train_labels, epochs=5, validation_data=(valid_imgs, valid_labels), shuffle=True, verbose=v )\n",
        "      \n",
        "    print(\"training loss calculations for \"+ limp)\n",
        "    loss_tr, accuracy_tr =model.evaluate(x=train_imgs, y=train_labels, batch_size=128, verbose=v)\n",
        "    print(\"calculating validation loss for \"+ limp)\n",
        "    loss_val, accuracy_val =model.evaluate(x=valid_imgs, y=valid_labels, batch_size=128, verbose=v)\n",
        "    accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "    print(accuracies)\n",
        "  return accuracies\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxKTrlBCt5S9",
        "colab_type": "code",
        "outputId": "88d296bf-4519-4314-b6ad-e2c67f153239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        }
      },
      "cell_type": "code",
      "source": [
        "#R=evaluate_limps(limps=[\"SHOULDER\"],augment=True)\n",
        "R=evaluate_limps(limps=[\"SHOULDER\"],augment=False)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2821 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "reading studies of SHOULDER\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2821/2821 [00:00<00:00, 18675.19it/s]\n",
            "100%|██████████| 194/194 [00:00<00:00, 18559.71it/s]\n",
            "  0%|          | 25/8379 [00:00<00:34, 245.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8379,)\n",
            "(563,)\n",
            "reading SHOULDER training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8379/8379 [00:35<00:00, 238.89it/s]\n",
            "  5%|▍         | 26/563 [00:00<00:02, 252.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8379, 224, 224, 1)\n",
            "reading SHOULDER validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 563/563 [00:02<00:00, 244.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(563, 224, 224, 1)\n",
            "(8379, 224, 224, 1) (8379,)\n",
            "making model\n",
            "compiling\n",
            "fitting\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-283b1d12c06f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_limps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SHOULDER\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-ab9d5bfbd5d6>\u001b[0m in \u001b[0;36mevaluate_limps\u001b[0;34m(model_no, imagenet, freeze_all, v, limps, augment)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training loss calculations for \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mlimp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_4 to have shape (224, 224, 3) but got array with shape (224, 224, 1)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MMcyBZz_yDvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(R).head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7i_zlgSuKyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#may need to make a function to evaluate all limps over one model together as one dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_9VVa8XVLMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Current point: generalizing functions and code cleaning+ seeing early stopping callback**"
      ]
    },
    {
      "metadata": {
        "id": "vt5njgtKmMOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All unsolved problems:\n",
        "\n",
        "* which layers to freeze and which to train + should I train TL before freezing it ?\n",
        "* data augmentation to generate more data (solved by khaled,still needs little verification)\n",
        "* recording variation in accuracy after every change to get intuition\n",
        "* what does outputted loss represent ? how to read the number ?\n",
        "* normalization step and its effect on accuracy\n",
        "* should I use 1 or two neurons at output layer ?\n",
        "* binary crossentropy weights\n",
        "* justifying parameter use and discovering useful params\n",
        "* try training with model unfrozen with imagenet and without it\n",
        "* make a function to record and tabulate outputs\n",
        "* could we add precision or recall metric ? change accuracy?\n",
        "* why doesn't it work if removed GlobalAveragePooling line?\n",
        "* see if want to freeze less layers\n",
        "* generalize file reading functions\n",
        "* use better batch size ? increase epochs ?\n",
        "* training function with preprocessing flag and multiple model comparisons\n",
        "* grid search like function to tune models and hyperparameters\n",
        "* early stopping callback and save model to drive for continuing training\n",
        "* feature concatenation\n",
        "* #Khaled : the runtime died issue when trying to fit the generated data first before using it in fit_generaton which gives a warning in trainig(will try to avoid by falsing featurewise_centre and ZCA and using another parameteres that doesn't need the mean so doesn't need fit at all)\n",
        "\n",
        "\n",
        "* #Khaled : Grayscale creates an error because the pretrained models are expected (x,y,3) shape not (x,y,1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IVMVHTzRF7f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**results on wrist data: no preprocessing but resize**\n",
        "\n",
        "\n",
        "1- Densenet, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "2- Densenet with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "65% train and 65% validation\n",
        "\n",
        "3-InceptionV3, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "4-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "82% train and 78% validation\n",
        "\n",
        "\n",
        "\n",
        "**results on wrist data: with keras preprocessing **\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them)\n",
        "71% train and 68% validation\n",
        "\n",
        "epoch=6 mins\n",
        "\n",
        "2- Densenet WITHOUT imagenet and didnt freeze \n",
        "\n",
        "62% train and 59% validation\n",
        "\n",
        "epoch=5.5 mins\n",
        "\n",
        "3-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "79% train and 74% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "4-InceptionV3 WITHOUT imagenet and didnt freeze \n",
        "59% train and 57% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "**results on Shoulder data: no preprocessing \n",
        "**\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them) 53.57% train and 51.15% validation\n",
        "\n",
        "epoch=6 mins\n",
        "\n",
        "**results DATA AUGMENTATION on Shoulder data:no preprocessing \n",
        "**\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them) 48.9% train and 50.2% validation\n",
        "\n",
        "epoch=6.7 mins\n",
        "\n",
        "\n",
        "** Conclusions till now **\n",
        "\n",
        "1- keras preprocessing (nomalization and subtracting mean maybe ) with inception reduces  accuracy both training and validation , but with DenseNet it improved accuracy \n",
        "\n",
        "2- Training over imagenet weights without freezing the TL model gives significantly higher accuracy than freezing whole TL and just training denses\n",
        "\n",
        "3-Training TL without imagenet and no freeze till now (2 Trials) seems to get worse results than with imagenet and training over it"
      ]
    }
  ]
}