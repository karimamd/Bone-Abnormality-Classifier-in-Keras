{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MURA Clean Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Corv113DVLKx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ]
    },
    {
      "metadata": {
        "id": "SmaRONjkdPJc",
        "colab_type": "code",
        "outputId": "8c4cfa91-e34c-42f3-94b2-224f1f470de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MURA-v1.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cfvs-0PJVXGi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# !wget -c https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "# !unzip MURA-v1.1.zip\n",
        "# !rm MURA-v1.1.zip\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hezUpQAcVLK6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from tqdm import tqdm\n",
        "import keras\n",
        "pd.options.display.max_colwidth = 100\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenetv2 import preprocess_input\n",
        "from keras.applications import MobileNet\n",
        "from keras.callbacks import (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard)\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
        "from keras.metrics import binary_accuracy, binary_crossentropy\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenetv2 import MobileNetV2\n",
        "from keras.preprocessing import image as k_im_prep\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ut2PF0_mOa_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Reading**"
      ]
    },
    {
      "metadata": {
        "id": "dBWtYEWrOeOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paths_n_labels(csv,str_limp):\n",
        "    #make dataframe\n",
        "    studies=pd.read_csv(csv, sep=',',header=None)\n",
        "    #separate study paths and labels of given limp from those of other limps\n",
        "    limp_studies=studies[studies[0].str.contains(str_limp)==True]\n",
        "    #make it a numpy\n",
        "    limp_studies=np.array(limp_studies)\n",
        "    #limp study folder paths\n",
        "    limp_paths=[]\n",
        "    #labels of given limp\n",
        "    limp_labels=[]\n",
        "    for i in tqdm( range(limp_studies.shape[0]) ):\n",
        "        study_path=limp_studies[i][0]\n",
        "        study_label=limp_studies[i][1]\n",
        "        study_files = [f for f in listdir(study_path) if isfile(join(study_path, f))]\n",
        "        for image in study_files:\n",
        "            limp_paths.append(study_path + image)\n",
        "            limp_labels.append(study_label)\n",
        "\n",
        "    limp_paths=np.array(limp_paths)\n",
        "    limp_labels=np.array(limp_labels)\n",
        "\n",
        "    return limp_paths,limp_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d-KrxmIReoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#general function with options for wrist data case, only set wrist_train=True in the case of wrist training data only\n",
        "#for all other limps and for validation data even that of wrist just pass the paths\n",
        "#targ_size is image resizing with default(224,224)\n",
        "#preprocess flag is for using keras preprocessing for images or just resizing\n",
        "def read_images(paths ,targ_size= (224, 224), wrist_train=False, preprocess=False):\n",
        "    images=[]\n",
        "    #load any limp images\n",
        "    if(not wrist_train):\n",
        "        for path in tqdm(paths):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "#             img = np.stack((img,)*3, axis=-1)\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    #special case for wrist train corrupted data       \n",
        "    else:\n",
        "        #did this because it gave an error at sample  5307 or near it if took all\n",
        "        sample_e=5307\n",
        "        sample_s2=5339\n",
        "        images=[]\n",
        "        for path in tqdm(paths[:sample_e]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "#             img = np.stack((img,)*3, axis=-1)\n",
        "\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "\n",
        "        #new start\n",
        "        for path in tqdm(paths[sample_s2:]):\n",
        "            img=k_im_prep.load_img(path, target_size=targ_size)\n",
        "#             img = np.stack((img,)*3, axis=-1)\n",
        "\n",
        "            if(preprocess):\n",
        "                img = k_im_prep.img_to_array(img)\n",
        "                img = np.expand_dims(img, axis=0)\n",
        "                img = preprocess_input(img)\n",
        "                images.append(np.array(img)[0])\n",
        "            else:\n",
        "              images.append(np.array(img))\n",
        "    \n",
        "    \n",
        "    #making it a numpy array instead of python list\n",
        "    return (np.array(images))\n",
        "  \n",
        "\n",
        "def wrist_labels(labels):\n",
        "  sample_e=5307\n",
        "  sample_s2=5339\n",
        "  return np.hstack( [ labels[:sample_e], labels[sample_s2:] ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9xFCfDa2aR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_studies='MURA-v1.1/train_labeled_studies.csv'\n",
        "valid_studies='MURA-v1.1/valid_labeled_studies.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4QggWXq2iu-",
        "colab_type": "code",
        "outputId": "589d7039-62e7-40bb-d6e4-fde396f52504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#training images paths and labels\n",
        "#starting with wrist data\n",
        "train_paths,train_labels=paths_n_labels(train_studies,\"WRIST\")\n",
        "#data bias : train\n",
        "print(\"0 normal, 1 abnormal\")\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3460/3460 [00:00<00:00, 19254.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 normal, 1 abnormal\n",
            "{0: 5769, 1: 3987}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "58kzFDVzGzzl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vJ_NG3xJOjfU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(\"0 normal, 1 abnormal\")\n",
        "# unique, counts = np.unique(valid_labels, return_counts=True)\n",
        "# print(dict(zip(unique, counts)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1u2CNQ7VLMC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ]
    },
    {
      "metadata": {
        "id": "mMST4D272Whd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_FT_model(base=1, imagenet=True, freeze_all=True, add_denses=True):\n",
        "  \n",
        "  #weights of pretrained model\n",
        "  if (imagenet==True):\n",
        "    w='imagenet'\n",
        "  else:\n",
        "    w=None\n",
        "  \n",
        "  #initializing pretrained model\n",
        "  if (base==0):\n",
        "    base_model = MobileNetV2(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 1):\n",
        "    base_model = DenseNet169(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 2):\n",
        "    base_model = InceptionV3(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "  elif (base == 3):\n",
        "    base_model = ResNet50(input_shape= (224, 224, 3),weights=w, include_top=False)   \n",
        "  elif (base == 4):\n",
        "    base_model = NASNetLarge(input_shape= (224, 224, 3),weights=w, include_top=False)\n",
        "    \n",
        " \n",
        "  if (freeze_all):\n",
        "    #freeze layers of densenet\n",
        "    for layer in base_model.layers:\n",
        "      layer.trainable= False \n",
        "  \n",
        "  # add a global spatial average pooling layer\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "  if(add_denses):\n",
        "    # let's add a fully-connected layer\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "\n",
        "    # and a logistic layer -- let's say we have 200 classes\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "    # this is the model we will train\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    \n",
        "  else:\n",
        "    # just feature extractor\n",
        "    model = Model(inputs=base_model.input, output=x)\n",
        "  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1VohqXoheEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def images_n_labels(limp): \n",
        "  \n",
        "  print(\"\\nreading studies of \"+ limp + \"\\n\")\n",
        "  print(train_studies)\n",
        "  train_paths,train_labels=paths_n_labels(train_studies,limp)\n",
        "  valid_paths,valid_labels=paths_n_labels(valid_studies,limp)\n",
        "  \n",
        "  print(train_labels.shape)\n",
        "  print(valid_labels.shape)\n",
        "  print(\"reading \"+ limp + \" training images\")\n",
        "  if (limp == \"WRIST\"):\n",
        "    train_labels=wrist_labels(train_labels) #for wrist reading problem\n",
        "    train_imgs= read_images(train_paths,preprocess=True, wrist_train=True)\n",
        "    \n",
        "  else:\n",
        "    train_imgs= read_images(train_paths,preprocess=True, wrist_train=False)\n",
        "  print(train_imgs.shape)  \n",
        "  print(\"reading \"+ limp + \" validation images\")\n",
        "  valid_imgs= read_images(valid_paths,preprocess=True)\n",
        "  \n",
        "  print(\"Training data shape is \",train_imgs.shape,train_labels.shape) \n",
        "  print(\"Validation data shape is\" ,valid_imgs.shape,valid_labels.shape)\n",
        "\n",
        "  \n",
        "  return train_imgs, train_labels, valid_imgs, valid_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghAljjlsYGgf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The  Generator**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YX4l8VYbVLM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(  rescale=1./255,\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    zoom_range=0.1,\n",
        "    channel_shift_range=0.,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize a list of sample image data in the range of 0 to 1\n",
        "    : return: Numpy array of normalized data\n",
        "    \"\"\"\n",
        "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
        "  \n",
        "def show_images(images, cols = 1, titles = None):\n",
        "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
        "    \n",
        "    Parameters\n",
        "    ---------\n",
        "    images: List of np.arrays compatible with plt.imshow.\n",
        "    \n",
        "    cols (Default = 1): Number of columns in figure (number of rows is \n",
        "                        set to np.ceil(n_images/float(cols))).\n",
        "    \n",
        "    titles: List of titles corresponding to each image. Must have\n",
        "            the same length as titles.\n",
        "    \"\"\"\n",
        "#     image=normalize(image)\n",
        "    assert((titles is None)or (len(images) == len(titles)))\n",
        "    n_images = 5\n",
        "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
        "    fig = plt.figure()\n",
        "    for n, (image, title) in enumerate(zip(images, titles)):\n",
        "        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
        "        if image.ndim == 2:\n",
        "            plt.gray()\n",
        "            \n",
        "        plt.imshow(image)\n",
        "        a.set_title(title)\n",
        "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
        "    plt.show()\n",
        "    \n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CzAs7Sl8zKAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_images_from_data_gen(datagen):\n",
        "  datagen.fit(valid_imgs)\n",
        "  \n",
        "  max_num_iterations=1\n",
        "  num_iterations=0\n",
        "  \n",
        "  for X_batch, y_batch in datagen.flow(valid_imgs,valid_labels,batch_size=10):\n",
        "    if num_iterations >= max_num_iterations:\n",
        "      break\n",
        "    \n",
        "    for i in range(0,9):\n",
        "      plt.subplot(330+1+i)\n",
        "      X_batch[i]=normalize(X_batch[i])\n",
        "      plt.imshow(X_batch[i],cmap=plt.get_cmap('gray'))\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    num_iterations +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FweDqRialw67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read_single_img = image.load_img(\"MURA-v1.1/train/XR_SHOULDER/patient00001/study1_positive/image3.png\", target_size=(224, 224))\n",
        "# # img=normalize(img)\n",
        "# # plt.imshow(img)\n",
        "\n",
        "# # x_augmented =datagen.flow(img, label, batch_size=4)\n",
        "# # print(\"our next stop is runtime dying\")\n",
        "# # plot_images(x_augmented, \"Augmented Images\")\n",
        "\n",
        "# train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(\"HUMERUS\")\n",
        "# print(\"our next stop is error\")\n",
        "# show_images_from_data_gen(datagen)\n",
        "# x_augmented =datagen.flow(valid_imgs, valid_labels, batch_size=4)\n",
        "# print(\"our next stop is runtime dying\")\n",
        "# # plot_images(x_augmented, \"Augmented Images\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uR-g2EsQcnmm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generalizing over all limps"
      ]
    },
    {
      "metadata": {
        "id": "eXYxHrw_cqIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limps=[\"SHOULDER\", \"WRIST\", \"FINGER\", \"ELBOW\", \"HUMERUS\", \"HAND\", \"FOREARM\"]\n",
        "\n",
        "batch_aug_size=64\n",
        "#pass to model list of limps because if wanted to train on less\n",
        "#function outputs a dictionary or dataframe has train and val accuracies for each limp using a chosen model\n",
        "\n",
        "def evaluate_limps(model_no=1, imagenet=True, freeze_all=False,v=1 , limps=limps, augment=False):\n",
        "  accuracies={}\n",
        "  for limp in limps:\n",
        "    #print(\"reading \"+ limp + \" images\\n\")\n",
        "    train_imgs, train_labels, valid_imgs, valid_labels= images_n_labels(limp)\n",
        "    print(\"making model\")\n",
        "    model=make_FT_model(base= model_no, imagenet=imagenet, freeze_all=freeze_all, add_denses=True)\n",
        "    print(\"compiling\")\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    ##############################################################\n",
        "    \n",
        "    if(augment): #dataaugmentation model fitting\n",
        "      print(\"Augmenting Input data\")\n",
        "      # # compute quantities required for featurewise normalization\n",
        "      # # (std, mean, and principal components if ZCA whitening is applied)\n",
        "\n",
        "      datagen.fit(train_imgs)\n",
        "      model.fit_generator(datagen.flow(train_imgs, train_labels, batch_size=batch_aug_size),\n",
        "                    steps_per_epoch=len(train_imgs) / (batch_aug_size), epochs=5,use_multiprocessing=False,workers=6,validation_data=(valid_imgs, valid_labels))\n",
        "      print(\"training augmentation calculations for \"+ limp)\n",
        "\n",
        "      loss_tr, accuracy_tr =model.evaluate_generator(datagen.flow(train_imgs,train_labels), use_multiprocessing=True,steps=len(valid_imgs) / batch_aug_size)\n",
        "                                                     \n",
        "      print(\"calculating validation augmentation loss for \"+ limp)\n",
        "                                                     \n",
        "      loss_val, accuracy_val = model.evaluate_generator(datagen.flow(valid_imgs,valid_labels), use_multiprocessing=True,steps=len(valid_imgs) / batch_aug_size)\n",
        "      \n",
        "      accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "      \n",
        "      print(model.predict(valid_imgs, batch_size=batch_aug_size))\n",
        "      print(accuracies)\n",
        "\n",
        "    ###############################################################\n",
        "    else:\n",
        "      \n",
        "      print(\"fitting\")\n",
        "      model.fit(train_imgs, train_labels, epochs=5, validation_data=(valid_imgs, valid_labels), shuffle=True, verbose=v )\n",
        "    \n",
        "    y_pred=model.predict(valid_imgs)\n",
        "#     print(y_pred)\n",
        "    y_pred=np.argmax(y_pred, axis=1)\n",
        "  \n",
        "    y_pred_for_training=model.predict(valid_imgs)\n",
        "    y_pred_for_training=np.argmax(y_pred_for_training, axis=1)\n",
        "\n",
        "  \n",
        "    print(\"second prediction vlayes\")\n",
        "    print(y_pred)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cnf_matrix = confusion_matrix(valid_labels, y_pred)\n",
        "    \n",
        "    cnf_matrix2 = confusion_matrix(train_labels, y_pred)\n",
        "\n",
        "    np.set_printoptions(precision=2)\n",
        "    \n",
        "    cm_class_label = ['Normal','abnormal']\n",
        "    # Plot non-normalized confusion matrix\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(cnf_matrix,cm_class_label,title='Confusion matrix for valiadation set')\n",
        "    plot_confusion_matrix(cnf_matrix2,cm_class_label,title='Confusion matrix for training set')\n",
        "\n",
        "    \n",
        "    print(\"training loss calculations for \"+ limp)\n",
        "    loss_tr, accuracy_tr =model.evaluate(x=train_imgs, y=train_labels, batch_size=128, verbose=v)\n",
        "    print(\"calculating validation loss for \"+ limp)\n",
        "    loss_val, accuracy_val =model.evaluate(x=valid_imgs, y=valid_labels, batch_size=128, verbose=v)\n",
        "    accuracies.update( {limp : [accuracy_tr, accuracy_val]} )\n",
        "    print(accuracies)\n",
        "    \n",
        "\n",
        "  return accuracies\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxKTrlBCt5S9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1391
        },
        "outputId": "653b8f93-8e72-43a5-bdee-3812435db6b7"
      },
      "cell_type": "code",
      "source": [
        "# R=evaluate_limps(limps=[\"HUMERUS\"],augment=True)\n",
        "R=evaluate_limps(limps=[\"HUMERUS\"],augment=False)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 592/592 [00:00<00:00, 20524.79it/s]\n",
            "100%|██████████| 135/135 [00:00<00:00, 20229.76it/s]\n",
            "  1%|▏         | 17/1272 [00:00<00:07, 167.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "reading studies of HUMERUS\n",
            "\n",
            "MURA-v1.1/train_labeled_studies.csv\n",
            "(1272,)\n",
            "(288,)\n",
            "reading HUMERUS training images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1272/1272 [00:07<00:00, 173.59it/s]\n",
            "  7%|▋         | 21/288 [00:00<00:01, 203.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1272, 224, 224, 3)\n",
            "reading HUMERUS validation images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 288/288 [00:01<00:00, 159.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training data shape is  (1272, 224, 224, 3) (1272,)\n",
            "Validation data shape is (288, 224, 224, 3) (288,)\n",
            "making model\n",
            "compiling\n",
            "fitting\n",
            "Train on 1272 samples, validate on 288 samples\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-b41086a7e37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_limps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HUMERUS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-b0f6619915c1>\u001b[0m in \u001b[0;36mevaluate_limps\u001b[0;34m(model_no, imagenet, freeze_all, v, limps, augment)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitting\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv1/conv_1/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@train...propFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_2/RMSprop/gradients/conv1/conv_1/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv1/conv_1/kernel/read)]]\n\t [[{{node loss_2/mul/_17861}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_49200_loss_2/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MMcyBZz_yDvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(R).head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7i_zlgSuKyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#may need to make a function to evaluate all limps over one model together as one dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_9VVa8XVLMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Current point: generalizing functions and code cleaning+ seeing early stopping callback**"
      ]
    },
    {
      "metadata": {
        "id": "vt5njgtKmMOc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All unsolved problems:\n",
        "\n",
        "* which layers to freeze and which to train + should I train TL before freezing it ?\n",
        "* data augmentation to generate more data (solved by khaled,still needs little verification)\n",
        "* recording variation in accuracy after every change to get intuition\n",
        "* what does outputted loss represent ? how to read the number ?\n",
        "* normalization step and its effect on accuracy\n",
        "* should I use 1 or two neurons at output layer ?\n",
        "* binary crossentropy weights\n",
        "* justifying parameter use and discovering useful params\n",
        "* try training with model unfrozen with imagenet and without it\n",
        "* make a function to record and tabulate outputs\n",
        "* could we add precision or recall metric ? change accuracy?\n",
        "* why doesn't it work if removed GlobalAveragePooling line?\n",
        "* see if want to freeze less layers\n",
        "* generalize file reading functions\n",
        "* use better batch size ? increase epochs ?\n",
        "* training function with preprocessing flag and multiple model comparisons\n",
        "* grid search like function to tune models and hyperparameters\n",
        "* early stopping callback and save model to drive for continuing training\n",
        "* feature concatenation\n",
        "* #Khaled : the runtime died issue when trying to fit the generated data first before using it in fit_generaton which gives a warning in trainig(will try to avoid by falsing featurewise_centre and ZCA and using another parameteres that doesn't need the mean so doesn't need fit at all)\n",
        "\n",
        "\n",
        "* #Khaled : Grayscale creates an error because the pretrained models are expected (x,y,3) shape not (x,y,1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IVMVHTzRF7f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**results on wrist data: no preprocessing but resize**\n",
        "\n",
        "\n",
        "1- Densenet, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "2- Densenet with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "65% train and 65% validation\n",
        "\n",
        "3-InceptionV3, with imagenet and froze all at rms prop\n",
        "\n",
        "59% train 55% val\n",
        "\n",
        "4-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "\n",
        "82% train and 78% validation\n",
        "\n",
        "\n",
        "\n",
        "**results on wrist data: with keras preprocessing **\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them)\n",
        "71% train and 68% validation\n",
        "\n",
        "epoch=6 mins\n",
        "\n",
        "2- Densenet WITHOUT imagenet and didnt freeze \n",
        "\n",
        "62% train and 59% validation\n",
        "\n",
        "epoch=5.5 mins\n",
        "\n",
        "3-InceptionV3 with imagenet and didnt freeze (trained over them)\n",
        "79% train and 74% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "4-InceptionV3 WITHOUT imagenet and didnt freeze \n",
        "59% train and 57% validation\n",
        "\n",
        "epoch=5 mins\n",
        "\n",
        "**results on Shoulder data: no preprocessing \n",
        "**\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them) 53.57% train and 51.15% validation\n",
        "\n",
        "epoch=6 mins\n",
        "\n",
        "2-InceptionV3 with Imagenet and didn't freeze (trained over them) 50% train and 50% validation\n",
        "\n",
        "**results DATA AUGMENTATION on Shoulder data:no preprocessing \n",
        "**\n",
        "\n",
        "1- Densenet with imagenet and didnt freeze (trained over them) 48.9% train and 50.2% validation\n",
        "\n",
        "epoch=6.7 mins\n",
        "\n",
        "\n",
        "** Conclusions till now **\n",
        "\n",
        "1- keras preprocessing (nomalization and subtracting mean maybe ) with inception reduces  accuracy both training and validation , but with DenseNet it improved accuracy \n",
        "\n",
        "2- Training over imagenet weights without freezing the TL model gives significantly higher accuracy than freezing whole TL and just training denses\n",
        "\n",
        "3-Training TL without imagenet and no freeze till now (2 Trials) seems to get worse results than with imagenet and training over it"
      ]
    }
  ]
}